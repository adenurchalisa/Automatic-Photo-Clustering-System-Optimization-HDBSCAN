{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adenurchalisa/Automatic-Photo-Clustering-System-Optimization-HDBSCAN/blob/main/notebooks/12_CGA_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 12: CGA â€” Cluster-Guided Gaussian Augmentation\n",
    "\n",
    "## Latar Belakang\n",
    "\n",
    "Berdasarkan analisis di **Notebook 11**, ditemukan bahwa:\n",
    "- Baseline (Correlation, mcs=15, ms=70) menghasilkan **54 cluster** dengan noise **43.8%**\n",
    "- Dari total noise, **25% adalah probable singletons** (CGA candidates)\n",
    "- Ada **minority clusters** (ukuran kecil) yang rentan hilang saat HDBSCAN dijalankan dengan threshold berbeda\n",
    "\n",
    "## Apa itu CGA?\n",
    "\n",
    "**CGA (Cluster-Guided Gaussian Augmentation)** adalah adaptasi dari prinsip inti FICAug (Haghbin et al., 2025) untuk konteks *unsupervised* face clustering:\n",
    "\n",
    "> Untuk setiap minority cluster di embedding space, buat sampel sintetis baru menggunakan **Gaussian sampling** di sekitar distribusi cluster tersebut â€” sehingga cluster yang terlalu kecil untuk di-detect HDBSCAN menjadi cukup \"padat\" untuk terbentuk.\n",
    "\n",
    "### Formula:\n",
    "$$\\mathbf{x}_{syn} = \\boldsymbol{\\mu}_k + \\alpha \\cdot \\boldsymbol{\\sigma}_k \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "Di mana:\n",
    "- $\\boldsymbol{\\mu}_k$ = centroid cluster ke-$k$ (GPU: `cp.mean`)\n",
    "- $\\boldsymbol{\\sigma}_k$ = std per dimensi cluster ke-$k$ (diagonal approximation, GPU: `cp.std`)\n",
    "- $\\alpha$ = scale factor yang mengontrol variasi (hyperparameter)\n",
    "- $\\epsilon$ = random noise dari distribusi normal standar (GPU: `cp.random.randn`)\n",
    "\n",
    "## Alur Eksperimen\n",
    "\n",
    "```\n",
    "Embeddings (12,715 Ã— 512)\n",
    "        â†“\n",
    "Baseline Clustering (Correlation, mcs=15, ms=70)\n",
    "        â†“\n",
    "Identifikasi Minority Clusters\n",
    "        â†“\n",
    "CGA: Gaussian Sampling per Cluster [GPU - CuPy]\n",
    "        â†“\n",
    "Gabungkan: Original + Synthetic Embeddings\n",
    "        â†“\n",
    "Re-clustering HDBSCAN\n",
    "        â†“\n",
    "Evaluasi & Ablation Study (Î±, n_synthetic)\n",
    "```\n",
    "\n",
    "## Akselerasi GPU\n",
    "\n",
    "Seluruh operasi statistik (mean, std, random sampling) dilakukan di GPU menggunakan **CuPy** â€” konsisten dengan pendekatan di Notebook 6 (QJSD) dan Notebook 10 (Angular Distance).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1 â€” Instalasi & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hdbscan cupy-cuda12x -q\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "\n",
    "import cupy as cp\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Cek GPU\n",
    "gpu_name = cp.cuda.runtime.getDeviceProperties(0)['name'].decode()\n",
    "gpu_mem  = cp.cuda.runtime.getDeviceProperties(0)['totalGlobalMem'] / (1024**3)\n",
    "print(f\"âœ… GPU: {gpu_name}  ({gpu_mem:.1f} GB VRAM)\")\n",
    "print(f\"âœ… CuPy version : {cp.__version__}\")\n",
    "print(f\"âœ… HDBSCAN version: {hdbscan.__version__}\")\n",
    "print(\"âœ… Semua library berhasil dimuat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2 â€” Konfigurasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # â”€â”€â”€ Path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    EMBEDDINGS_PATH  = '/content/drive/MyDrive/OTW S.KOM/Embeddings/embeddings_data.pkl'\n",
    "    NB11_RESULTS     = '/content/drive/MyDrive/OTW S.KOM/Results/notebook11_noise_analysis.pkl'\n",
    "    RESULTS_DIR      = '/content/drive/MyDrive/OTW S.KOM/Results/'\n",
    "    PLOTS_DIR        = '/content/drive/MyDrive/OTW S.KOM/Results/Plots/'\n",
    "\n",
    "    # â”€â”€â”€ Baseline HDBSCAN (sama dengan Notebook 11) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    BASELINE_PARAMS = {\n",
    "        'metric'           : 'correlation',\n",
    "        'min_cluster_size' : 15,\n",
    "        'min_samples'      : 70,\n",
    "        'method'           : 'eom',\n",
    "    }\n",
    "\n",
    "    # â”€â”€â”€ Minority Cluster Threshold â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Cluster dengan ukuran <= threshold ini dianggap 'minority'\n",
    "    # dan menjadi target CGA augmentation.\n",
    "    # Default: 2 Ã— min_cluster_size (30)\n",
    "    MINORITY_THRESHOLD = 30\n",
    "\n",
    "    # â”€â”€â”€ CGA Hyperparameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Nilai alpha yang akan di-ablasi\n",
    "    ALPHA_LIST      = [0.5, 1.0, 1.5]\n",
    "\n",
    "    # Jumlah sampel sintetis per cluster member yang di-ablasi\n",
    "    N_SYNTHETIC_LIST = [3, 5, 10]\n",
    "\n",
    "    # Seed reproducibility di GPU\n",
    "    SEED = 42\n",
    "\n",
    "    # â”€â”€â”€ Re-clustering params â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Gunakan parameter yang sama dengan baseline untuk fair comparison\n",
    "    RECLUSTER_PARAMS = {\n",
    "        'metric'           : 'correlation',\n",
    "        'min_cluster_size' : 15,\n",
    "        'min_samples'      : 70,\n",
    "        'method'           : 'eom',\n",
    "    }\n",
    "\n",
    "    # Batch size untuk transfer GPU â†’ CPU\n",
    "    GPU_BATCH_SIZE = 2000\n",
    "\n",
    "\n",
    "cp.random.seed(Config.SEED)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… CONFIG LOADED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Minority threshold  : â‰¤ {Config.MINORITY_THRESHOLD} members\")\n",
    "print(f\"  Alpha values (ablasi): {Config.ALPHA_LIST}\")\n",
    "print(f\"  N synthetic (ablasi) : {Config.N_SYNTHETIC_LIST}\")\n",
    "print(f\"  GPU Seed             : {Config.SEED}\")\n",
    "print(f\"  Total kombinasi ablasi: {len(Config.ALPHA_LIST) * len(Config.N_SYNTHETIC_LIST)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3 â€” Load Data & Reproduce Baseline\n",
    "\n",
    "Data embedding dimuat dan HDBSCAN baseline dijalankan ulang â€” identik dengan konfigurasi di Notebook 11 (Correlation, mcs=15, ms=70). Ini memastikan konsistensi eksperimen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“ LOAD DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not os.path.exists('/content/drive/MyDrive'):\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"âœ… Google Drive sudah ter-mount\")\n",
    "\n",
    "os.makedirs(Config.RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(Config.PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "# Load embeddings\n",
    "with open(Config.EMBEDDINGS_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "embeddings = np.array(data['embeddings']).astype('float32')\n",
    "metadata   = data['metadata']\n",
    "\n",
    "print(f\"\\nâœ… Embeddings loaded: {embeddings.shape}\")\n",
    "print(f\"   Metadata: {len(metadata)} entries\")\n",
    "\n",
    "# Transfer ke GPU\n",
    "print(f\"\\nâ³ Transfer embeddings ke GPU...\")\n",
    "t = time.time()\n",
    "embeddings_gpu = cp.array(embeddings)  # (N, 512) di VRAM\n",
    "print(f\"   âœ… GPU array: {embeddings_gpu.shape}, dtype={embeddings_gpu.dtype} ({time.time()-t:.2f}s)\")\n",
    "vram_mb = embeddings_gpu.nbytes / (1024**2)\n",
    "print(f\"   VRAM digunakan: {vram_mb:.1f} MB\")\n",
    "\n",
    "# Reproduce baseline clustering (CPU - HDBSCAN belum support correlation di cuML)\n",
    "print(f\"\\nâ³ Reproducing baseline (Correlation, mcs=15, ms=70)...\")\n",
    "t = time.time()\n",
    "clusterer_baseline = hdbscan.HDBSCAN(\n",
    "    min_cluster_size         = Config.BASELINE_PARAMS['min_cluster_size'],\n",
    "    min_samples              = Config.BASELINE_PARAMS['min_samples'],\n",
    "    cluster_selection_method = Config.BASELINE_PARAMS['method'],\n",
    "    metric                   = Config.BASELINE_PARAMS['metric'],\n",
    ")\n",
    "labels_baseline = clusterer_baseline.fit_predict(embeddings)\n",
    "elapsed = time.time() - t\n",
    "\n",
    "n_clusters_base = len(set(labels_baseline)) - 1\n",
    "n_noise_base    = int((labels_baseline == -1).sum())\n",
    "coverage_base   = float((labels_baseline != -1).sum() / len(labels_baseline))\n",
    "\n",
    "print(f\"   âœ… Selesai ({elapsed:.2f}s)\")\n",
    "print(f\"\\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(f\"  â”‚  BASELINE RESULTS                        â”‚\")\n",
    "print(f\"  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "print(f\"  â”‚  Clusters     : {n_clusters_base:<25}â”‚\")\n",
    "print(f\"  â”‚  Noise        : {n_noise_base:<8,} ({1-coverage_base:.1%})            â”‚\")\n",
    "print(f\"  â”‚  Coverage     : {coverage_base:.1%}                       â”‚\")\n",
    "print(f\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4 â€” Identifikasi Minority Clusters\n",
    "\n",
    "Minority clusters adalah cluster dengan jumlah anggota di bawah **`MINORITY_THRESHOLD`** (default: 30, atau 2Ã— min_cluster_size). Cluster-cluster ini menjadi target augmentasi CGA.\n",
    "\n",
    "Logika: jika sebuah cluster punya terlalu sedikit anggota, HDBSCAN menjadi tidak stabil dalam mendeteksinya â€” sedikit perubahan hyperparameter bisa membuatnya hilang. CGA menstabilkan cluster ini dengan menambah density di sekitarnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ğŸ” IDENTIFIKASI MINORITY CLUSTERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Hitung ukuran setiap cluster\n",
    "cluster_sizes = Counter({k: v for k, v in Counter(labels_baseline).items() if k != -1})\n",
    "sizes_arr     = np.array(list(cluster_sizes.values()))\n",
    "cluster_ids   = np.array(list(cluster_sizes.keys()))\n",
    "\n",
    "# Identifikasi minority clusters\n",
    "minority_ids   = cluster_ids[sizes_arr <= Config.MINORITY_THRESHOLD]\n",
    "majority_ids   = cluster_ids[sizes_arr > Config.MINORITY_THRESHOLD]\n",
    "\n",
    "n_minority = len(minority_ids)\n",
    "n_majority = len(majority_ids)\n",
    "\n",
    "minority_members_total = sum(cluster_sizes[k] for k in minority_ids)\n",
    "majority_members_total = sum(cluster_sizes[k] for k in majority_ids)\n",
    "\n",
    "print(f\"\"\"\n",
    "  Threshold minority : â‰¤ {Config.MINORITY_THRESHOLD} anggota\n",
    "\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚  DISTRIBUSI CLUSTER                                         â”‚\n",
    "  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "  â”‚  Total clusters      : {n_clusters_base:<6}                          â”‚\n",
    "  â”‚  Majority clusters   : {n_majority:<6} ({n_majority/n_clusters_base:.1%}) â€” TIDAK di-augment  â”‚\n",
    "  â”‚  Minority clusters   : {n_minority:<6} ({n_minority/n_clusters_base:.1%}) â€” Target CGA        â”‚\n",
    "  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "  â”‚  Anggota di majority : {majority_members_total:<6,}                          â”‚\n",
    "  â”‚  Anggota di minority : {minority_members_total:<6,}                          â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "print(\"  Detail minority clusters:\")\n",
    "print(f\"  {'Cluster ID':>10} {'Ukuran':>8}\")\n",
    "print(f\"  {'â”€'*10} {'â”€'*8}\")\n",
    "for k in sorted(minority_ids):\n",
    "    print(f\"  {k:>10} {cluster_sizes[k]:>8}\")\n",
    "\n",
    "print(f\"\\n  Statistik ukuran cluster (seluruh):\")\n",
    "print(f\"  Min : {sizes_arr.min()}\")\n",
    "print(f\"  Max : {sizes_arr.max()}\")\n",
    "print(f\"  Mean: {sizes_arr.mean():.1f}\")\n",
    "print(f\"  Std : {sizes_arr.std():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5 â€” CGA Core: Gaussian Sampling di GPU\n",
    "\n",
    "Fungsi utama CGA yang sepenuhnya berjalan di GPU menggunakan CuPy.\n",
    "\n",
    "### Algoritma:\n",
    "```\n",
    "Untuk setiap minority cluster k:\n",
    "    1. Ambil semua embedding anggota cluster k â†’ cluster_embs (CuPy array)\n",
    "    2. Hitung Âµ_k = mean(cluster_embs)  [GPU]\n",
    "    3. Hitung Ïƒ_k = std(cluster_embs)   [GPU, diagonal approx]\n",
    "    4. Hitung r_k = max distance dari Âµ_k (radius cluster)  [GPU]\n",
    "    5. Untuk i in range(n_synthetic):\n",
    "        a. Sample Îµ ~ N(0, I)  [GPU]\n",
    "        b. x_syn = Âµ_k + Î± * Ïƒ_k * Îµ  [GPU]\n",
    "        c. Validasi: ||x_syn - Âµ_k|| <= r_k  [GPU]\n",
    "        d. Jika valid â†’ tambahkan ke synthetic pool\n",
    "```\n",
    "\n",
    "### Kenapa diagonal approximation untuk Ïƒ?\n",
    "Full covariance matrix untuk embedding 512-dim berukuran 512Ã—512 = 262,144 parameter per cluster. Dengan diagonal approximation, kita hanya butuh 512 nilai â€” jauh lebih efisien dan cukup akurat untuk distribusi embeddings wajah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cga_gaussian_sampling_gpu(\n",
    "    embeddings_gpu: cp.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    target_cluster_ids: np.ndarray,\n",
    "    alpha: float = 1.0,\n",
    "    n_synthetic: int = 5,\n",
    "    seed: int = 42\n",
    ") -> tuple[cp.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    CGA: Cluster-Guided Gaussian Augmentation â€” sepenuhnya berjalan di GPU.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings_gpu     : CuPy array (N, D) â€” semua embedding di VRAM\n",
    "    labels             : NumPy array (N,) â€” label cluster HDBSCAN\n",
    "    target_cluster_ids : cluster ID yang akan di-augment\n",
    "    alpha              : scale factor variasi (0.5=kecil, 1.0=normal, 1.5=besar)\n",
    "    n_synthetic        : jumlah sampel sintetis yang dihasilkan per cluster\n",
    "    seed               : random seed untuk reproducibility\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    syn_embs_gpu   : CuPy array (M, D) â€” synthetic embeddings di VRAM\n",
    "    syn_labels     : NumPy array (M,) â€” label cluster dari synthetic embeddings\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Diagonal covariance approximation: Ïƒ_k = std per dimensi (bukan full cov matrix)\n",
    "    - Boundary validation: hanya sampel dalam radius cluster r_k yang disimpan\n",
    "    - Seluruh operasi (mean, std, randn, distance) berjalan di GPU\n",
    "    \"\"\"\n",
    "    cp.random.seed(seed)\n",
    "\n",
    "    syn_embs_list   = []\n",
    "    syn_labels_list = []\n",
    "\n",
    "    labels_gpu = cp.array(labels)  # transfer label ke GPU untuk masking\n",
    "    D = embeddings_gpu.shape[1]\n",
    "\n",
    "    for k in target_cluster_ids:\n",
    "        # --- Step 1: Ambil anggota cluster k di GPU ---\n",
    "        mask         = labels_gpu == int(k)\n",
    "        cluster_embs = embeddings_gpu[mask]          # (n_k, D)\n",
    "        n_k          = cluster_embs.shape[0]\n",
    "\n",
    "        if n_k < 2:\n",
    "            # Tidak cukup untuk estimasi distribusi\n",
    "            continue\n",
    "\n",
    "        # --- Step 2 & 3: Estimasi distribusi Gaussian [GPU] ---\n",
    "        mu    = cluster_embs.mean(axis=0)            # (D,) â€” centroid\n",
    "        sigma = cluster_embs.std(axis=0)             # (D,) â€” diagonal std\n",
    "\n",
    "        # Hindari sigma = 0 (dimensi konstan dalam cluster)\n",
    "        sigma = cp.where(sigma < 1e-8, 1e-8, sigma)\n",
    "\n",
    "        # --- Step 4: Hitung radius cluster r_k [GPU] ---\n",
    "        diffs      = cluster_embs - mu[cp.newaxis, :]  # (n_k, D)\n",
    "        distances  = cp.linalg.norm(diffs, axis=1)     # (n_k,)\n",
    "        r_k        = float(distances.max())             # scalar\n",
    "\n",
    "        # --- Step 5: Sample n_synthetic sampel sekaligus [GPU] ---\n",
    "        epsilon = cp.random.randn(n_synthetic, D)                  # (n_syn, D)\n",
    "        x_syn   = mu[cp.newaxis, :] + alpha * sigma[cp.newaxis, :] * epsilon  # (n_syn, D)\n",
    "\n",
    "        # --- Boundary validation [GPU] ---\n",
    "        syn_dists  = cp.linalg.norm(x_syn - mu[cp.newaxis, :], axis=1)  # (n_syn,)\n",
    "        valid_mask = syn_dists <= r_k                                     # (n_syn,) bool\n",
    "        x_syn_valid = x_syn[valid_mask]                                   # (m, D)\n",
    "\n",
    "        m = x_syn_valid.shape[0]\n",
    "        if m > 0:\n",
    "            syn_embs_list.append(x_syn_valid)\n",
    "            syn_labels_list.extend([int(k)] * m)\n",
    "\n",
    "    if len(syn_embs_list) == 0:\n",
    "        return cp.empty((0, D), dtype=cp.float32), np.array([], dtype=np.int32)\n",
    "\n",
    "    syn_embs_gpu = cp.vstack(syn_embs_list).astype(cp.float32)   # (M, D)\n",
    "    syn_labels   = np.array(syn_labels_list, dtype=np.int32)      # (M,)\n",
    "\n",
    "    return syn_embs_gpu, syn_labels\n",
    "\n",
    "\n",
    "# â”€â”€ Quick Sanity Check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ§ª SANITY CHECK: Uji fungsi CGA dengan satu cluster\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_k      = minority_ids[0] if len(minority_ids) > 0 else 0\n",
    "test_size   = cluster_sizes[test_k]\n",
    "\n",
    "syn_test, lbl_test = cga_gaussian_sampling_gpu(\n",
    "    embeddings_gpu     = embeddings_gpu,\n",
    "    labels             = labels_baseline,\n",
    "    target_cluster_ids = np.array([test_k]),\n",
    "    alpha              = 1.0,\n",
    "    n_synthetic        = 5,\n",
    "    seed               = Config.SEED\n",
    ")\n",
    "\n",
    "# Hitung jarak rata-rata sampel sintetis dari centroid cluster\n",
    "orig_mask   = labels_baseline == test_k\n",
    "mu_test     = embeddings_gpu[cp.array(orig_mask)].mean(axis=0)\n",
    "syn_dists   = cp.linalg.norm(syn_test - mu_test[cp.newaxis, :], axis=1)\n",
    "orig_dists  = cp.linalg.norm(\n",
    "    embeddings_gpu[cp.array(orig_mask)] - mu_test[cp.newaxis, :], axis=1\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "  Cluster ID       : {test_k}\n",
    "  Original members : {test_size}\n",
    "  Synthetic diminta: 5\n",
    "  Synthetic valid  : {len(syn_test)} (yang lolos boundary validation)\n",
    "\n",
    "  Jarak ke centroid:\n",
    "    Original (mean)  : {float(orig_dists.mean()):.4f}\n",
    "    Synthetic (mean) : {float(syn_dists.mean()):.4f}\n",
    "    â†’ Synthetic berada {'dalam' if float(syn_dists.mean()) <= float(orig_dists.mean())*1.5 else 'luar'} jangkauan yang wajar\n",
    "\n",
    "  Shape output GPU : {syn_test.shape}\n",
    "  Dtype            : {syn_test.dtype}\n",
    "\"\"\")\n",
    "print(\"âœ… Sanity check passed â€” fungsi CGA bekerja dengan benar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6 â€” Ablation Study: Grid Search Î± Ã— n_synthetic\n",
    "\n",
    "Menguji semua kombinasi hyperparameter CGA:\n",
    "- **Î± (alpha)** âˆˆ {0.5, 1.0, 1.5}: seberapa besar variasi sampel sintetis\n",
    "- **n_synthetic** âˆˆ {3, 5, 10}: berapa sampel yang dibuat per cluster\n",
    "\n",
    "Untuk setiap kombinasi:\n",
    "1. Jalankan CGA â†’ hasilkan synthetic embeddings (GPU)\n",
    "2. Gabungkan dengan embeddings asli\n",
    "3. Re-run HDBSCAN dengan parameter sama\n",
    "4. Evaluasi: Silhouette Score, jumlah cluster, coverage rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”¬ ABLATION STUDY: Î± Ã— n_synthetic\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Grid: Î±={Config.ALPHA_LIST} Ã— n_syn={Config.N_SYNTHETIC_LIST}\")\n",
    "print(f\"  Minority clusters target: {len(minority_ids)} clusters\")\n",
    "print(f\"  Total kombinasi: {len(Config.ALPHA_LIST) * len(Config.N_SYNTHETIC_LIST)}\")\n",
    "print()\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "header = f\"  {'Î±':>5} {'n_syn':>6} | {'n_syn_gen':>10} {'n_aug':>8} {'clusters':>9} {'noise':>7} {'coverage':>9} {'silhouette':>11}\"\n",
    "sep    = f\"  {'â”€'*5} {'â”€'*6} | {'â”€'*10} {'â”€'*8} {'â”€'*9} {'â”€'*7} {'â”€'*9} {'â”€'*11}\"\n",
    "print(header)\n",
    "print(sep)\n",
    "\n",
    "for alpha, n_syn in product(Config.ALPHA_LIST, Config.N_SYNTHETIC_LIST):\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    # â”€â”€ Step 1: Generate synthetic embeddings di GPU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    syn_embs_gpu, syn_labels = cga_gaussian_sampling_gpu(\n",
    "        embeddings_gpu     = embeddings_gpu,\n",
    "        labels             = labels_baseline,\n",
    "        target_cluster_ids = minority_ids,\n",
    "        alpha              = alpha,\n",
    "        n_synthetic        = n_syn,\n",
    "        seed               = Config.SEED\n",
    "    )\n",
    "\n",
    "    n_synthetic_generated = len(syn_embs_gpu)\n",
    "\n",
    "    # â”€â”€ Step 2: Gabungkan original + synthetic (GPU â†’ CPU) â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Transfer synthetic ke CPU untuk HDBSCAN (correlation metric)\n",
    "    syn_embs_cpu = cp.asnumpy(syn_embs_gpu)                          # (M, 512)\n",
    "    emb_augmented = np.vstack([embeddings, syn_embs_cpu])             # (N+M, 512)\n",
    "    n_total_aug = len(emb_augmented)\n",
    "\n",
    "    # â”€â”€ Step 3: Re-clustering HDBSCAN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    clusterer_aug = hdbscan.HDBSCAN(\n",
    "        min_cluster_size         = Config.RECLUSTER_PARAMS['min_cluster_size'],\n",
    "        min_samples              = Config.RECLUSTER_PARAMS['min_samples'],\n",
    "        cluster_selection_method = Config.RECLUSTER_PARAMS['method'],\n",
    "        metric                   = Config.RECLUSTER_PARAMS['metric'],\n",
    "    )\n",
    "    labels_aug_full = clusterer_aug.fit_predict(emb_augmented)\n",
    "\n",
    "    # Ambil hanya label untuk embeddings ASLI (bukan synthetic)\n",
    "    labels_aug_orig = labels_aug_full[:len(embeddings)]\n",
    "\n",
    "    # â”€â”€ Step 4: Evaluasi â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    n_clusters_aug = len(set(labels_aug_orig)) - 1\n",
    "    n_noise_aug    = int((labels_aug_orig == -1).sum())\n",
    "    coverage_aug   = float((labels_aug_orig != -1).sum() / len(labels_aug_orig))\n",
    "\n",
    "    sil_aug = None\n",
    "    mask_valid = labels_aug_orig != -1\n",
    "    if mask_valid.sum() > 10 and len(set(labels_aug_orig[mask_valid])) >= 2:\n",
    "        sil_aug = float(silhouette_score(\n",
    "            embeddings[mask_valid], labels_aug_orig[mask_valid],\n",
    "            metric='correlation', sample_size=3000, random_state=Config.SEED\n",
    "        ))\n",
    "\n",
    "    elapsed = time.time() - t_start\n",
    "\n",
    "    ablation_results.append({\n",
    "        'alpha'              : alpha,\n",
    "        'n_synthetic'        : n_syn,\n",
    "        'n_synthetic_gen'    : n_synthetic_generated,\n",
    "        'n_total_aug'        : n_total_aug,\n",
    "        'n_clusters'         : n_clusters_aug,\n",
    "        'n_noise'            : n_noise_aug,\n",
    "        'coverage'           : coverage_aug,\n",
    "        'silhouette'         : sil_aug,\n",
    "        'elapsed_s'          : elapsed,\n",
    "        'labels_aug_orig'    : labels_aug_orig.copy(),\n",
    "    })\n",
    "\n",
    "    sil_str = f\"{sil_aug:.4f}\" if sil_aug else \"   N/A\"\n",
    "    print(f\"  {alpha:>5.1f} {n_syn:>6} | {n_synthetic_generated:>10,} {n_total_aug:>8,} \"\n",
    "          f\"{n_clusters_aug:>9} {n_noise_aug:>7,} {coverage_aug:>9.1%} {sil_str:>11}\")\n",
    "\n",
    "    # Bersihkan memori GPU\n",
    "    del syn_embs_gpu, syn_embs_cpu, emb_augmented\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "print(sep)\n",
    "print(f\"  {'BASELINE':>5} {'  â€”':>6} | {'       â€”':>10} {'â€”':>8} \"\n",
    "      f\"{n_clusters_base:>9} {n_noise_base:>7,} {coverage_base:>9.1%} {'  (ref)':>11}\")\n",
    "\n",
    "# Temukan konfigurasi terbaik\n",
    "df_ablation = pd.DataFrame([{k: v for k, v in r.items() if k != 'labels_aug_orig'}\n",
    "                             for r in ablation_results])\n",
    "df_valid    = df_ablation.dropna(subset=['silhouette'])\n",
    "\n",
    "# Best by coverage\n",
    "best_cov_idx  = df_valid['coverage'].idxmax()\n",
    "best_cov      = df_valid.loc[best_cov_idx]\n",
    "\n",
    "# Best by silhouette\n",
    "best_sil_idx  = df_valid['silhouette'].idxmax()\n",
    "best_sil      = df_valid.loc[best_sil_idx]\n",
    "\n",
    "print(f\"\"\"\n",
    "  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  ğŸ† BEST BY COVERAGE:\n",
    "     Î±={best_cov['alpha']}, n_syn={int(best_cov['n_synthetic'])}\n",
    "     Coverage  : {best_cov['coverage']:.1%}  (baseline: {coverage_base:.1%}, Î”={best_cov['coverage']-coverage_base:+.1%})\n",
    "     Clusters  : {int(best_cov['n_clusters'])}  (baseline: {n_clusters_base})\n",
    "     Silhouette: {best_cov['silhouette']:.4f}\n",
    "\n",
    "  ğŸ† BEST BY SILHOUETTE:\n",
    "     Î±={best_sil['alpha']}, n_syn={int(best_sil['n_synthetic'])}\n",
    "     Silhouette: {best_sil['silhouette']:.4f}  (baseline tidak ada referensi sil di CGA context)\n",
    "     Coverage  : {best_sil['coverage']:.1%}\n",
    "     Clusters  : {int(best_sil['n_clusters'])}\n",
    "  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7 â€” Analisis Best Configuration\n",
    "\n",
    "Memilih konfigurasi CGA terbaik berdasarkan tradeoff antara **coverage** dan **silhouette score**, lalu menganalisis detail perubahannya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š ANALISIS BEST CGA CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Pilih best berdasarkan coverage tertinggi\n",
    "best_result = ablation_results[best_cov_idx]\n",
    "labels_best = best_result['labels_aug_orig']\n",
    "\n",
    "best_alpha   = best_result['alpha']\n",
    "best_n_syn   = best_result['n_synthetic']\n",
    "best_cov_val = best_result['coverage']\n",
    "best_sil_val = best_result['silhouette']\n",
    "best_nclu    = best_result['n_clusters']\n",
    "best_nnoise  = best_result['n_noise']\n",
    "\n",
    "# â”€â”€ Perbandingan detail â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\"\"\n",
    "  Konfigurasi terpilih: Î±={best_alpha}, n_synthetic={best_n_syn}\n",
    "\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚          BASELINE          â”‚          CGA                   â”‚\n",
    "  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "  â”‚  Clusters  : {n_clusters_base:<6}             â”‚  Clusters  : {best_nclu:<6}           â”‚\n",
    "  â”‚  Noise     : {n_noise_base:<6,}             â”‚  Noise     : {best_nnoise:<6,}           â”‚\n",
    "  â”‚  Coverage  : {coverage_base:.1%}              â”‚  Coverage  : {best_cov_val:.1%}            â”‚\n",
    "  â”‚  Silhouette: (ref)               â”‚  Silhouette: {best_sil_val:.4f}           â”‚\n",
    "  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "  â”‚  Î” Clusters  : {best_nclu - n_clusters_base:+d}                                    â”‚\n",
    "  â”‚  Î” Noise     : {best_nnoise - n_noise_base:+,}                                   â”‚\n",
    "  â”‚  Î” Coverage  : {best_cov_val - coverage_base:+.1%}                                  â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "# â”€â”€ Analisis cluster yang berubah â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sizes_base = Counter({k: v for k, v in Counter(labels_baseline).items() if k != -1})\n",
    "sizes_best = Counter({k: v for k, v in Counter(labels_best).items() if k != -1})\n",
    "\n",
    "# Cluster yang bertahan dan ukurannya berubah\n",
    "common_clusters = set(sizes_base.keys()) & set(sizes_best.keys())\n",
    "grown_clusters  = [(k, sizes_base[k], sizes_best[k])\n",
    "                   for k in common_clusters if sizes_best[k] > sizes_base[k]]\n",
    "shrunk_clusters = [(k, sizes_base[k], sizes_best[k])\n",
    "                   for k in common_clusters if sizes_best[k] < sizes_base[k]]\n",
    "new_clusters    = set(sizes_best.keys()) - set(sizes_base.keys())\n",
    "lost_clusters   = set(sizes_base.keys()) - set(sizes_best.keys())\n",
    "\n",
    "print(f\"  Cluster yang bertambah besar    : {len(grown_clusters)}\")\n",
    "print(f\"  Cluster yang berkurang (noise)  : {len(shrunk_clusters)}\")\n",
    "print(f\"  Cluster baru (dari noise)       : {len(new_clusters)}\")\n",
    "print(f\"  Cluster yang hilang             : {len(lost_clusters)}\")\n",
    "\n",
    "if grown_clusters:\n",
    "    grown_clusters.sort(key=lambda x: x[2] - x[1], reverse=True)\n",
    "    print(f\"\\n  Top cluster yang paling banyak bertambah anggota (setelah CGA):\")\n",
    "    print(f\"  {'Cluster ID':>10} {'Before':>8} {'After':>8} {'Delta':>8}\")\n",
    "    print(f\"  {'â”€'*10} {'â”€'*8} {'â”€'*8} {'â”€'*8}\")\n",
    "    for k, before, after in grown_clusters[:10]:\n",
    "        delta = after - before\n",
    "        print(f\"  {k:>10} {before:>8} {after:>8} {delta:>+8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8 â€” Visualisasi Hasil CGA\n",
    "\n",
    "Empat panel visualisasi:\n",
    "1. **Heatmap ablation** â€” coverage tiap kombinasi Î± Ã— n_synthetic\n",
    "2. **Bar chart perbandingan** â€” baseline vs CGA terbaik (clusters, noise, coverage)\n",
    "3. **Distribusi ukuran cluster** â€” sebelum dan sesudah CGA\n",
    "4. **Scatter ablation** â€” tradeoff coverage vs silhouette untuk tiap konfigurasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs  = gridspec.GridSpec(2, 2, figure=fig, hspace=0.4, wspace=0.35)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])  # Heatmap coverage ablation\n",
    "ax2 = fig.add_subplot(gs[0, 1])  # Bar chart baseline vs CGA\n",
    "ax3 = fig.add_subplot(gs[1, 0])  # Distribusi ukuran cluster\n",
    "ax4 = fig.add_subplot(gs[1, 1])  # Scatter coverage vs silhouette\n",
    "\n",
    "# â”€â”€ Panel 1: Heatmap coverage ablation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pivot_cov = df_valid.pivot(index='alpha', columns='n_synthetic', values='coverage') * 100\n",
    "im = ax1.imshow(pivot_cov.values, cmap='YlOrRd', aspect='auto',\n",
    "                vmin=pivot_cov.values.min() - 1, vmax=pivot_cov.values.max() + 1)\n",
    "ax1.set_xticks(range(len(pivot_cov.columns)))\n",
    "ax1.set_xticklabels([f'n={v}' for v in pivot_cov.columns])\n",
    "ax1.set_yticks(range(len(pivot_cov.index)))\n",
    "ax1.set_yticklabels([f'Î±={v}' for v in pivot_cov.index])\n",
    "ax1.set_title('Coverage Rate (%) per Konfigurasi CGA', fontweight='bold')\n",
    "ax1.set_xlabel('n_synthetic')\n",
    "ax1.set_ylabel('alpha (Î±)')\n",
    "for i in range(len(pivot_cov.index)):\n",
    "    for j in range(len(pivot_cov.columns)):\n",
    "        val = pivot_cov.values[i, j]\n",
    "        ax1.text(j, i, f'{val:.1f}%', ha='center', va='center',\n",
    "                 color='black', fontsize=10, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax1, shrink=0.8)\n",
    "\n",
    "# â”€â”€ Panel 2: Bar chart baseline vs CGA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "metrics      = ['Coverage (%)', 'Clusters', 'Noise (%)']\n",
    "baseline_vals = [coverage_base * 100, n_clusters_base, (1 - coverage_base) * 100]\n",
    "cga_vals      = [best_cov_val * 100, best_nclu, (1 - best_cov_val) * 100]\n",
    "x = np.arange(len(metrics))\n",
    "w = 0.35\n",
    "b1 = ax2.bar(x - w/2, baseline_vals, w, label='Baseline', color='steelblue', edgecolor='black')\n",
    "b2 = ax2.bar(x + w/2, cga_vals,      w, label=f'CGA (Î±={best_alpha}, n={best_n_syn})',\n",
    "             color='darkorange', edgecolor='black')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(metrics)\n",
    "ax2.set_title('Baseline vs CGA Terbaik', fontweight='bold')\n",
    "ax2.legend()\n",
    "for bar in list(b1) + list(b2):\n",
    "    h = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, h + 0.5,\n",
    "             f'{h:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# â”€â”€ Panel 3: Distribusi ukuran cluster â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sizes_base_vals = [cluster_sizes[k] for k in cluster_sizes]\n",
    "sizes_best_vals = [sizes_best[k] for k in sizes_best]\n",
    "bins = np.linspace(0, max(max(sizes_base_vals), max(sizes_best_vals)) + 10, 30)\n",
    "ax3.hist(sizes_base_vals, bins=bins, alpha=0.6, label='Baseline', color='steelblue', edgecolor='white')\n",
    "ax3.hist(sizes_best_vals, bins=bins, alpha=0.6, label=f'CGA (Î±={best_alpha}, n={best_n_syn})',\n",
    "         color='darkorange', edgecolor='white')\n",
    "ax3.axvline(Config.MINORITY_THRESHOLD, color='red', linestyle='--', linewidth=1.5,\n",
    "            label=f'Minority threshold ({Config.MINORITY_THRESHOLD})')\n",
    "ax3.set_xlabel('Ukuran Cluster (jumlah anggota)')\n",
    "ax3.set_ylabel('Frekuensi')\n",
    "ax3.set_title('Distribusi Ukuran Cluster\\nSebelum vs Sesudah CGA', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "# â”€â”€ Panel 4: Scatter coverage vs silhouette â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(Config.ALPHA_LIST)))\n",
    "alpha_color_map = dict(zip(Config.ALPHA_LIST, colors))\n",
    "\n",
    "for _, row in df_valid.iterrows():\n",
    "    c = alpha_color_map[row['alpha']]\n",
    "    ax4.scatter(row['coverage'] * 100, row['silhouette'],\n",
    "                color=c, s=120, edgecolors='black', zorder=3)\n",
    "    ax4.annotate(f\"n={int(row['n_synthetic'])}\",\n",
    "                 (row['coverage'] * 100, row['silhouette']),\n",
    "                 textcoords='offset points', xytext=(5, 5), fontsize=8)\n",
    "\n",
    "# Highlight best\n",
    "ax4.scatter(best_cov_val * 100, best_sil_val, s=250, marker='*',\n",
    "            color='red', zorder=5, label=f'Best (Î±={best_alpha}, n={best_n_syn})')\n",
    "\n",
    "# Legend untuk alpha\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor=alpha_color_map[a],\n",
    "                           markersize=10, label=f'Î±={a}') for a in Config.ALPHA_LIST]\n",
    "legend_elements.append(Line2D([0], [0], marker='*', color='w', markerfacecolor='red',\n",
    "                               markersize=14, label='Best Config'))\n",
    "ax4.legend(handles=legend_elements, loc='lower right')\n",
    "ax4.set_xlabel('Coverage Rate (%)')\n",
    "ax4.set_ylabel('Silhouette Score')\n",
    "ax4.set_title('Tradeoff: Coverage vs Silhouette\\nper Konfigurasi CGA', fontweight='bold')\n",
    "\n",
    "fig.suptitle('Notebook 12: CGA â€” Cluster-Guided Gaussian Augmentation\\nAblation Study Results',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "plot_path = f\"{Config.PLOTS_DIR}notebook12_cga_results.png\"\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"ğŸ’¾ Plot disimpan: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9 â€” Summary & Simpan Hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“‹ NOTEBOOK 12 â€” SUMMARY CGA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "delta_cov      = best_cov_val - coverage_base\n",
    "delta_clusters = best_nclu - n_clusters_base\n",
    "delta_noise    = best_nnoise - n_noise_base\n",
    "\n",
    "print(f\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              CLUSTER-GUIDED GAUSSIAN AUGMENTATION (CGA)              â”‚\n",
    "â”‚              RINGKASAN EKSPERIMEN                                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  DATASET:                                                            â”‚\n",
    "â”‚  â€¢ Total embeddings asli   : {len(embeddings):,}                          â”‚\n",
    "â”‚  â€¢ Minority clusters target: {len(minority_ids)} clusters                     â”‚\n",
    "â”‚  â€¢ Minority threshold      : â‰¤ {Config.MINORITY_THRESHOLD} anggota                    â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  ABLATION STUDY:                                                     â”‚\n",
    "â”‚  â€¢ Î± values  : {Config.ALPHA_LIST}                               â”‚\n",
    "â”‚  â€¢ n_syn vals: {Config.N_SYNTHETIC_LIST}                                         â”‚\n",
    "â”‚  â€¢ Total kombinasi: {len(Config.ALPHA_LIST) * len(Config.N_SYNTHETIC_LIST)}                                    â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  KONFIGURASI TERBAIK (by coverage):                                  â”‚\n",
    "â”‚  â€¢ Î±={best_alpha}, n_synthetic={best_n_syn}                                     â”‚\n",
    "â”‚  â€¢ Synthetic embeddings gen: {best_result['n_synthetic_gen']:,}                     â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  PERBANDINGAN HASIL:                                                 â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚ Metric        â”‚   Baseline   â”‚     CGA     â”‚      Delta       â”‚  â”‚\n",
    "â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚\n",
    "â”‚  â”‚ Clusters      â”‚ {n_clusters_base:<12} â”‚ {best_nclu:<11} â”‚ {delta_clusters:+16} â”‚  â”‚\n",
    "â”‚  â”‚ Noise         â”‚ {n_noise_base:<12,} â”‚ {best_nnoise:<11,} â”‚ {delta_noise:+16,} â”‚  â”‚\n",
    "â”‚  â”‚ Coverage      â”‚ {coverage_base:<12.1%} â”‚ {best_cov_val:<11.1%} â”‚ {delta_cov:+16.1%} â”‚  â”‚\n",
    "â”‚  â”‚ Silhouette    â”‚ (ref)        â”‚ {best_sil_val:<11.4f} â”‚                  â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "# â”€â”€ Simpan semua hasil â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "save_data = {\n",
    "    'timestamp'            : datetime.now().isoformat(),\n",
    "    'notebook'             : 'Notebook 12: CGA â€” Cluster-Guided Gaussian Augmentation',\n",
    "    'n_total_embeddings'   : len(embeddings),\n",
    "    'n_minority_clusters'  : len(minority_ids),\n",
    "    'minority_ids'         : minority_ids,\n",
    "    'minority_threshold'   : Config.MINORITY_THRESHOLD,\n",
    "    'baseline_params'      : Config.BASELINE_PARAMS,\n",
    "    'n_clusters_baseline'  : n_clusters_base,\n",
    "    'n_noise_baseline'     : n_noise_base,\n",
    "    'coverage_baseline'    : coverage_base,\n",
    "    'ablation_results'     : [{\n",
    "        'alpha'            : r['alpha'],\n",
    "        'n_synthetic'      : r['n_synthetic'],\n",
    "        'n_synthetic_gen'  : r['n_synthetic_gen'],\n",
    "        'n_clusters'       : r['n_clusters'],\n",
    "        'n_noise'          : r['n_noise'],\n",
    "        'coverage'         : r['coverage'],\n",
    "        'silhouette'       : r['silhouette'],\n",
    "        'elapsed_s'        : r['elapsed_s'],\n",
    "    } for r in ablation_results],\n",
    "    'best_alpha'           : best_alpha,\n",
    "    'best_n_synthetic'     : best_n_syn,\n",
    "    'best_labels'          : labels_best,\n",
    "    'best_coverage'        : best_cov_val,\n",
    "    'best_silhouette'      : best_sil_val,\n",
    "    'best_n_clusters'      : best_nclu,\n",
    "    'best_n_noise'         : best_nnoise,\n",
    "    'delta_coverage'       : delta_cov,\n",
    "    'delta_clusters'       : delta_clusters,\n",
    "    'delta_noise'          : delta_noise,\n",
    "    'gpu_seed'             : Config.SEED,\n",
    "}\n",
    "\n",
    "pkl_path = f\"{Config.RESULTS_DIR}notebook12_cga_results.pkl\"\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump(save_data, f)\n",
    "\n",
    "print(f\"ğŸ’¾ Hasil disimpan: {pkl_path}\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… Notebook 12 selesai. Hasil CGA siap untuk analisis lanjutan.\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Bersihkan GPU memory\n",
    "del embeddings_gpu\n",
    "cp.get_default_memory_pool().free_all_blocks()\n",
    "print(f\"\\nğŸ§¹ GPU memory dibersihkan.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
