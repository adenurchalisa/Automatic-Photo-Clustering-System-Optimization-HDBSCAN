{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOzthNvivi8LEagKMyaGq8y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adenurchalisa/Automatic-Photo-Clustering-System-Optimization-HDBSCAN/blob/main/notebooks/2_Eksperimen_HDBSCAN_hyperparameter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CMAy_BdRePn",
        "outputId": "0fc06afa-62fb-494e-f1f2-9b29f4a072ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” GPU Info:\n",
            "name, memory.total [MiB]\n",
            "Tesla T4, 15360 MiB\n",
            "\n",
            "âœ… Installation complete!\n",
            "ğŸ’» Using CPU multi-core mode (fast enough for our dataset)\n"
          ]
        }
      ],
      "source": [
        "# Cek GPU (untuk informasi saja)\n",
        "print(\"ğŸ” GPU Info:\")\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
        "\n",
        "# Install hdbscan dengan optimasi\n",
        "!pip install hdbscan -q\n",
        "\n",
        "print(\"\\nâœ… Installation complete!\")\n",
        "print(\"ğŸ’» Using CPU multi-core mode (fast enough for our dataset)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Clustering\n",
        "import hdbscan\n",
        "\n",
        "# Preprocessing & Evaluation\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Google Colab\n",
        "from google.colab import drive\n",
        "\n",
        "# Cek CPU cores\n",
        "import multiprocessing\n",
        "n_cores = multiprocessing.cpu_count()\n",
        "\n",
        "print(\"âœ… All libraries imported!\")\n",
        "print(f\"ğŸ’» Available CPU cores: {n_cores}\")\n",
        "print(f\"   HDBSCAN will use all {n_cores} cores for parallel processing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2s-3dAgRl8g",
        "outputId": "a9a11f98-a1de-442c-9d07-d20939e1eb82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All libraries imported!\n",
            "ğŸ’» Available CPU cores: 2\n",
            "   HDBSCAN will use all 2 cores for parallel processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmMZrSaHS_7F",
        "outputId": "8060d45b-1d5e-4f11-a96a-047293f2bc28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDINGS_PATH = \"/content/drive/MyDrive/OTW S.KOM/Embeddings/embeddings_data.pkl\"\n",
        "\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/OTW S.KOM/Eksperimen 2\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "PARAM_GRID = {\n",
        "    'min_cluster_size': [10, 15, 20, 25, 30, 40, 50, 75, 100],\n",
        "    'min_samples': [1, 3, 5, 10, 15, 20, 25, None],\n",
        "    'cluster_selection_method': ['eom', 'leaf']\n",
        "}\n",
        "\n",
        "total_combinations = (\n",
        "    len(PARAM_GRID['min_cluster_size']) *\n",
        "    len(PARAM_GRID['min_samples']) *\n",
        "    len(PARAM_GRID['cluster_selection_method'])\n",
        ")\n",
        "\n",
        "print(\"âœ… Configuration set!\")\n",
        "print(f\"   - Embeddings path: {EMBEDDINGS_PATH}\")\n",
        "print(f\"   - Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"\\nğŸ“Š Parameter Grid:\")\n",
        "print(f\"   - min_cluster_size: {PARAM_GRID['min_cluster_size']}\")\n",
        "print(f\"   - min_samples: {PARAM_GRID['min_samples']}\")\n",
        "print(f\"   - cluster_selection_method: {PARAM_GRID['cluster_selection_method']}\")\n",
        "print(f\"   - Total combinations per embedding type: {total_combinations}\")\n",
        "print(f\"   - Total experiments (original + normalized): {total_combinations * 2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5KV60gPTRFa",
        "outputId": "80af09bc-ceba-42a8-980c-5f48970739e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Configuration set!\n",
            "   - Embeddings path: /content/drive/MyDrive/OTW S.KOM/Embeddings/embeddings_data.pkl\n",
            "   - Output directory: /content/drive/MyDrive/OTW S.KOM/Eksperimen 2\n",
            "\n",
            "ğŸ“Š Parameter Grid:\n",
            "   - min_cluster_size: [10, 15, 20, 25, 30, 40, 50, 75, 100]\n",
            "   - min_samples: [1, 3, 5, 10, 15, 20, 25, None]\n",
            "   - cluster_selection_method: ['eom', 'leaf']\n",
            "   - Total combinations per embedding type: 144\n",
            "   - Total experiments (original + normalized): 288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embeddings(file_path):\n",
        "    \"\"\"Load embeddings dari file pickle.\"\"\"\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data['embeddings'], data['metadata'], data['stats']\n",
        "\n",
        "# Load\n",
        "print(\"ğŸ”„ Loading embeddings...\")\n",
        "embeddings, metadata, stats = load_embeddings(EMBEDDINGS_PATH)\n",
        "\n",
        "print(\"âœ… Loaded successfully!\")\n",
        "print(f\"\\nğŸ“Š Dataset:\")\n",
        "print(f\"   - Total faces: {embeddings.shape[0]:,}\")\n",
        "print(f\"   - Embedding dim: {embeddings.shape[1]}\")\n",
        "print(f\"   - Original images: {stats['total_images']:,}\")\n",
        "print(f\"   - Images with faces: {stats['images_with_faces']:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A32102nqU43A",
        "outputId": "274bdfbb-6f8d-4f91-afe9-e1f28b6afab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Loading embeddings...\n",
            "âœ… Loaded successfully!\n",
            "\n",
            "ğŸ“Š Dataset:\n",
            "   - Total faces: 12,715\n",
            "   - Embedding dim: 512\n",
            "   - Original images: 2,533\n",
            "   - Images with faces: 2,365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 6: Core Functions\n",
        "# ============================================================\n",
        "\n",
        "def run_hdbscan(embeddings, min_cluster_size, min_samples,\n",
        "                metric='euclidean', cluster_selection_method='eom'):\n",
        "    \"\"\"\n",
        "    Run HDBSCAN clustering dengan CPU multi-core.\n",
        "    \"\"\"\n",
        "    clusterer = hdbscan.HDBSCAN(\n",
        "        min_cluster_size=min_cluster_size,\n",
        "        min_samples=min_samples,\n",
        "        metric=metric,\n",
        "        cluster_selection_method=cluster_selection_method,\n",
        "        core_dist_n_jobs=-1,  # Semua CPU cores\n",
        "        algorithm='best'\n",
        "    )\n",
        "\n",
        "    labels = clusterer.fit_predict(embeddings)\n",
        "\n",
        "    # Stats\n",
        "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    n_noise = list(labels).count(-1)\n",
        "    noise_ratio = n_noise / len(labels) * 100\n",
        "\n",
        "    # Metrics\n",
        "    if n_clusters > 1:\n",
        "        mask = labels != -1\n",
        "        if mask.sum() > n_clusters:\n",
        "            sil = silhouette_score(embeddings[mask], labels[mask])\n",
        "            dbi = davies_bouldin_score(embeddings[mask], labels[mask])\n",
        "        else:\n",
        "            sil, dbi = -1, -1\n",
        "    else:\n",
        "        sil, dbi = -1, -1\n",
        "\n",
        "    return {\n",
        "        'labels': labels,\n",
        "        'n_clusters': n_clusters,\n",
        "        'n_noise': n_noise,\n",
        "        'noise_ratio': noise_ratio,\n",
        "        'silhouette_score': sil,\n",
        "        'davies_bouldin_index': dbi,\n",
        "        'params': {\n",
        "            'min_cluster_size': min_cluster_size,\n",
        "            'min_samples': min_samples,\n",
        "            'metric': metric,\n",
        "            'cluster_selection_method': cluster_selection_method\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def get_cluster_distribution(labels):\n",
        "    \"\"\"Get cluster size distribution.\"\"\"\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    df = pd.DataFrame({'cluster_id': unique, 'count': counts})\n",
        "    return df.sort_values('count', ascending=False).reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"âœ… Functions defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf8UcSEnVAkp",
        "outputId": "6e5c876f-12f1-49e8-9c6d-9e77b0892a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Functions defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize embeddings\n",
        "embeddings_normalized = normalize(embeddings, norm='l2')\n",
        "\n",
        "# Verify\n",
        "orig_norms = np.linalg.norm(embeddings, axis=1)\n",
        "norm_norms = np.linalg.norm(embeddings_normalized, axis=1)\n",
        "\n",
        "print(\"ğŸ“Š Embedding Comparison:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nğŸ”¹ ORIGINAL:\")\n",
        "print(f\"   L2 Norm range: [{orig_norms.min():.4f}, {orig_norms.max():.4f}]\")\n",
        "print(f\"   Mean: {embeddings.mean():.6f}, Std: {embeddings.std():.6f}\")\n",
        "\n",
        "print(f\"\\nğŸ”¹ NORMALIZED (L2):\")\n",
        "print(f\"   L2 Norm range: [{norm_norms.min():.4f}, {norm_norms.max():.4f}]\")\n",
        "print(f\"   Mean: {embeddings_normalized.mean():.6f}, Std: {embeddings_normalized.std():.6f}\")\n",
        "\n",
        "print(\"\\nâœ… Preprocessing complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aUHbbxiVU3d",
        "outputId": "044ad7fe-388f-40f2-c5d3-7a461d00b91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Embedding Comparison:\n",
            "==================================================\n",
            "\n",
            "ğŸ”¹ ORIGINAL:\n",
            "   L2 Norm range: [6.2935, 30.1433]\n",
            "   Mean: 0.003600, Std: 0.957149\n",
            "\n",
            "ğŸ”¹ NORMALIZED (L2):\n",
            "   L2 Norm range: [1.0000, 1.0000]\n",
            "   Mean: 0.000122, Std: 0.044194\n",
            "\n",
            "âœ… Preprocessing complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 8: Run Extended Grid Search\n",
        "# ============================================================\n",
        "\n",
        "def run_grid_search(embeddings, param_grid, embedding_type='original'):\n",
        "    \"\"\"Run grid search dengan progress tracking.\"\"\"\n",
        "    all_results = []\n",
        "\n",
        "    total = (len(param_grid['min_cluster_size']) *\n",
        "             len(param_grid['min_samples']) *\n",
        "             len(param_grid['cluster_selection_method']))\n",
        "\n",
        "    print(f\"\\nğŸ” GRID SEARCH: {embedding_type.upper()}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    start_time = time.time()\n",
        "    idx = 0\n",
        "    best_sil = -1\n",
        "    best_result = None\n",
        "\n",
        "    for mcs in param_grid['min_cluster_size']:\n",
        "        for ms in param_grid['min_samples']:\n",
        "            for csm in param_grid['cluster_selection_method']:\n",
        "                idx += 1\n",
        "                t0 = time.time()\n",
        "\n",
        "                result = run_hdbscan(embeddings, mcs, ms, 'euclidean', csm)\n",
        "                result['embedding_type'] = embedding_type\n",
        "                all_results.append(result)\n",
        "\n",
        "                # Track best\n",
        "                if result['silhouette_score'] > best_sil:\n",
        "                    best_sil = result['silhouette_score']\n",
        "                    best_result = result\n",
        "\n",
        "                # Progress\n",
        "                t1 = time.time() - t0\n",
        "                sil = result['silhouette_score']\n",
        "                sil_str = f\"{sil:.4f}\" if sil != -1 else \"N/A\"\n",
        "                star = \"â­\" if sil > 0.35 else \"  \"\n",
        "\n",
        "                print(f\"{star}[{idx:3d}/{total}] mcs={mcs:3d}, ms={str(ms):4s}, \"\n",
        "                      f\"csm={csm:4s} â†’ clusters={result['n_clusters']:3d}, \"\n",
        "                      f\"noise={result['noise_ratio']:5.1f}%, sil={sil_str} ({t1:.1f}s)\")\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\nâœ… Completed in {elapsed:.1f}s ({elapsed/60:.1f} min)\")\n",
        "    print(f\"ğŸ† Best Silhouette: {best_sil:.4f}\")\n",
        "\n",
        "    return all_results, best_result\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# RUN GRID SEARCH\n",
        "# ============================================================\n",
        "print(\"ğŸš€ EXPERIMENT 1 EXTENDED: Euclidean Distance\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"   Dataset: {embeddings.shape[0]:,} faces, {embeddings.shape[1]} dims\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Run 1: Original\n",
        "print(\"\\n\" + \"ğŸ“Š \"*15)\n",
        "print(\"ğŸ“Š RUN 1: ORIGINAL EMBEDDINGS\")\n",
        "print(\"ğŸ“Š \"*15)\n",
        "all_orig, best_orig = run_grid_search(embeddings, PARAM_GRID, 'original')\n",
        "\n",
        "# Run 2: Normalized\n",
        "print(\"\\n\" + \"ğŸ“Š \"*15)\n",
        "print(\"ğŸ“Š RUN 2: NORMALIZED EMBEDDINGS\")\n",
        "print(\"ğŸ“Š \"*15)\n",
        "all_norm, best_norm = run_grid_search(embeddings_normalized, PARAM_GRID, 'normalized')\n",
        "\n",
        "print(\"\\nâœ… All Grid Searches Complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "3Vj7iIIcVwNi",
        "outputId": "7e96c9a8-4be4-49bb-bef1-cd3f0b807938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ EXPERIMENT 1 EXTENDED: Euclidean Distance\n",
            "======================================================================\n",
            "   Dataset: 12,715 faces, 512 dims\n",
            "======================================================================\n",
            "\n",
            "ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š \n",
            "ğŸ“Š RUN 1: ORIGINAL EMBEDDINGS\n",
            "ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š ğŸ“Š \n",
            "\n",
            "ğŸ” GRID SEARCH: ORIGINAL\n",
            "======================================================================\n",
            "  [  1/144] mcs= 10, ms=1   , csm=eom  â†’ clusters=122, noise= 31.0%, sil=0.2259 (172.9s)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-790/2749777053.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸ“Š RUN 1: ORIGINAL EMBEDDINGS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸ“Š \"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mall_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPARAM_GRID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'original'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Run 2: Normalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-790/2749777053.py\u001b[0m in \u001b[0;36mrun_grid_search\u001b[0;34m(embeddings, param_grid, embedding_type)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_hdbscan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mall_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-790/3751950003.py\u001b[0m in \u001b[0;36mrun_hdbscan\u001b[0;34m(embeddings, min_cluster_size, min_samples, metric, cluster_selection_method)\u001b[0m\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclusterer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/hdbscan/hdbscan_.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1310\u001b[0m             \u001b[0mcluster\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \"\"\"\n\u001b[0;32m-> 1312\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/hdbscan/hdbscan_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_linkage_tree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_min_spanning_tree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m         ) = hdbscan(clean_data, **kwargs)\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"precomputed\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/hdbscan/hdbscan_.py\u001b[0m in \u001b[0;36mhdbscan\u001b[0;34m(X, min_cluster_size, min_samples, alpha, cluster_selection_epsilon, cluster_selection_persistence, max_cluster_size, metric, p, leaf_size, algorithm, memory, approx_min_span_tree, gen_min_span_tree, core_dist_n_jobs, cluster_selection_method, allow_single_cluster, match_reference_implementation, cluster_selection_epsilon_max, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0;31m# still debugging for now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m                 (single_linkage_tree, result_min_span_tree) = memory.cache(\n\u001b[0m\u001b[1;32m    856\u001b[0m                     \u001b[0m_hdbscan_prims_kdtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/hdbscan/hdbscan_.py\u001b[0m in \u001b[0;36m_hdbscan_prims_kdtree\u001b[0;34m(X, min_samples, alpha, metric, p, leaf_size, gen_min_span_tree, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# Get distance to kth nearest neighbour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     core_distances = tree.query(\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_samples\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdualtree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbreadth_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     )[0][:, -1].copy(order=\"C\")\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 9: Compare & Analyze Results\n",
        "# ============================================================\n",
        "\n",
        "# Combine results\n",
        "def to_dataframe(results):\n",
        "    rows = []\n",
        "    for r in results:\n",
        "        rows.append({\n",
        "            'embedding_type': r['embedding_type'],\n",
        "            'min_cluster_size': r['params']['min_cluster_size'],\n",
        "            'min_samples': r['params']['min_samples'],\n",
        "            'cluster_selection_method': r['params']['cluster_selection_method'],\n",
        "            'n_clusters': r['n_clusters'],\n",
        "            'noise_ratio': r['noise_ratio'],\n",
        "            'silhouette_score': r['silhouette_score'],\n",
        "            'davies_bouldin_index': r['davies_bouldin_index']\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df_all = pd.concat([to_dataframe(all_orig), to_dataframe(all_norm)], ignore_index=True)\n",
        "\n",
        "# ============================================================\n",
        "# COMPARISON\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“Š COMPARISON: ORIGINAL vs NORMALIZED\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n{'Metric':<25} {'Original':>15} {'Normalized':>15}\")\n",
        "print(\"-\"*55)\n",
        "print(f\"{'Best Silhouette':<25} {best_orig['silhouette_score']:>15.4f} {best_norm['silhouette_score']:>15.4f}\")\n",
        "print(f\"{'Best DBI':<25} {best_orig['davies_bouldin_index']:>15.4f} {best_norm['davies_bouldin_index']:>15.4f}\")\n",
        "print(f\"{'N Clusters':<25} {best_orig['n_clusters']:>15} {best_norm['n_clusters']:>15}\")\n",
        "print(f\"{'Noise Ratio':<25} {best_orig['noise_ratio']:>14.2f}% {best_norm['noise_ratio']:>14.2f}%\")\n",
        "print(f\"{'min_cluster_size':<25} {best_orig['params']['min_cluster_size']:>15} {best_norm['params']['min_cluster_size']:>15}\")\n",
        "print(f\"{'min_samples':<25} {str(best_orig['params']['min_samples']):>15} {str(best_norm['params']['min_samples']):>15}\")\n",
        "print(f\"{'CSM':<25} {best_orig['params']['cluster_selection_method']:>15} {best_norm['params']['cluster_selection_method']:>15}\")\n",
        "\n",
        "# Determine winner\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if best_norm['silhouette_score'] > best_orig['silhouette_score']:\n",
        "    winner, winner_result, winner_emb = 'NORMALIZED', best_norm, embeddings_normalized\n",
        "    diff = best_norm['silhouette_score'] - best_orig['silhouette_score']\n",
        "    print(f\"ğŸ† WINNER: NORMALIZED (+{diff:.4f} silhouette)\")\n",
        "else:\n",
        "    winner, winner_result, winner_emb = 'ORIGINAL', best_orig, embeddings\n",
        "    diff = best_orig['silhouette_score'] - best_norm['silhouette_score']\n",
        "    print(f\"ğŸ† WINNER: ORIGINAL (+{diff:.4f} silhouette)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Top 10\n",
        "print(\"\\nğŸ“Š TOP 10 CONFIGURATIONS:\")\n",
        "df_valid = df_all[df_all['silhouette_score'] != -1]\n",
        "print(df_valid.nlargest(10, 'silhouette_score').to_string(index=False))"
      ],
      "metadata": {
        "id": "SzqeevqHWAMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 10: Visualization\n",
        "# ============================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Extended Grid Search: Euclidean Distance', fontsize=14, fontweight='bold')\n",
        "\n",
        "df_valid = df_all[df_all['silhouette_score'] != -1]\n",
        "\n",
        "# Plot 1: Histogram\n",
        "ax1 = axes[0, 0]\n",
        "for et, c in [('original', 'steelblue'), ('normalized', 'coral')]:\n",
        "    subset = df_valid[df_valid['embedding_type'] == et]\n",
        "    ax1.hist(subset['silhouette_score'], bins=20, alpha=0.6, label=et.title(), color=c)\n",
        "ax1.axvline(best_orig['silhouette_score'], color='steelblue', ls='--', lw=2)\n",
        "ax1.axvline(best_norm['silhouette_score'], color='coral', ls='--', lw=2)\n",
        "ax1.set_xlabel('Silhouette Score')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_title('Silhouette Distribution')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Silhouette vs min_cluster_size\n",
        "ax2 = axes[0, 1]\n",
        "for et, c, m in [('original', 'steelblue', 'o'), ('normalized', 'coral', 's')]:\n",
        "    subset = df_valid[df_valid['embedding_type'] == et]\n",
        "    grouped = subset.groupby('min_cluster_size')['silhouette_score'].max()\n",
        "    ax2.plot(grouped.index, grouped.values, marker=m, color=c, label=et.title(), lw=2)\n",
        "ax2.set_xlabel('min_cluster_size')\n",
        "ax2.set_ylabel('Best Silhouette')\n",
        "ax2.set_title('Silhouette vs min_cluster_size')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Noise vs Silhouette\n",
        "ax3 = axes[1, 0]\n",
        "for et, c in [('original', 'steelblue'), ('normalized', 'coral')]:\n",
        "    subset = df_valid[df_valid['embedding_type'] == et]\n",
        "    ax3.scatter(subset['noise_ratio'], subset['silhouette_score'], alpha=0.5, label=et.title(), c=c)\n",
        "ax3.set_xlabel('Noise Ratio (%)')\n",
        "ax3.set_ylabel('Silhouette Score')\n",
        "ax3.set_title('Trade-off: Noise vs Silhouette')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Top 10 bar\n",
        "ax4 = axes[1, 1]\n",
        "top10 = df_valid.nlargest(10, 'silhouette_score')\n",
        "colors = ['steelblue' if t=='original' else 'coral' for t in top10['embedding_type']]\n",
        "ax4.barh(range(10), top10['silhouette_score'], color=colors)\n",
        "ax4.set_yticks(range(10))\n",
        "ax4.set_yticklabels([f\"mcs={r['min_cluster_size']}, ms={r['min_samples']}\" for _,r in top10.iterrows()], fontsize=8)\n",
        "ax4.set_xlabel('Silhouette Score')\n",
        "ax4.set_title('Top 10 Configurations')\n",
        "ax4.invert_yaxis()\n",
        "ax4.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plot_path = os.path.join(OUTPUT_DIR, 'grid_search_visualization.png')\n",
        "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "print(f\"ğŸ“Š Saved: {plot_path}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EZfc3LEvgvPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 11: Save Results\n",
        "# ============================================================\n",
        "\n",
        "# Save CSV\n",
        "csv_path = os.path.join(OUTPUT_DIR, \"experiment1_extended_results.csv\")\n",
        "df_all.to_csv(csv_path, index=False)\n",
        "print(f\"ğŸ’¾ CSV: {csv_path}\")\n",
        "\n",
        "# Save best result\n",
        "best_data = {\n",
        "    'winner': winner,\n",
        "    'best_params': winner_result['params'],\n",
        "    'labels': winner_result['labels'],\n",
        "    'n_clusters': winner_result['n_clusters'],\n",
        "    'noise_ratio': winner_result['noise_ratio'],\n",
        "    'silhouette_score': winner_result['silhouette_score'],\n",
        "    'davies_bouldin_index': winner_result['davies_bouldin_index'],\n",
        "    'embeddings': winner_emb,\n",
        "    'metadata': metadata,\n",
        "    'comparison': {\n",
        "        'original': {'silhouette': best_orig['silhouette_score'], 'params': best_orig['params']},\n",
        "        'normalized': {'silhouette': best_norm['silhouette_score'], 'params': best_norm['params']}\n",
        "    }\n",
        "}\n",
        "\n",
        "pkl_path = os.path.join(OUTPUT_DIR, \"experiment1_best_result.pkl\")\n",
        "with open(pkl_path, 'wb') as f:\n",
        "    pickle.dump(best_data, f)\n",
        "print(f\"ğŸ’¾ PKL: {pkl_path}\")\n",
        "\n",
        "# Final Summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“‹ FINAL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nğŸ† Winner: {winner}\")\n",
        "print(f\"\\nğŸ”§ Best Parameters:\")\n",
        "print(f\"   - min_cluster_size: {winner_result['params']['min_cluster_size']}\")\n",
        "print(f\"   - min_samples: {winner_result['params']['min_samples']}\")\n",
        "print(f\"   - cluster_selection_method: {winner_result['params']['cluster_selection_method']}\")\n",
        "print(f\"\\nğŸ“Š Results:\")\n",
        "print(f\"   - Clusters: {winner_result['n_clusters']}\")\n",
        "print(f\"   - Noise: {winner_result['noise_ratio']:.2f}%\")\n",
        "print(f\"   - Silhouette: {winner_result['silhouette_score']:.4f}\")\n",
        "print(f\"   - DBI: {winner_result['davies_bouldin_index']:.4f}\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nâœ… EXPERIMENT 1 EXTENDED COMPLETE!\")"
      ],
      "metadata": {
        "id": "M7t3ZfcrgxMQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}