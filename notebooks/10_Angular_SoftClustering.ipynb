{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adenurchalisa/Automatic-Photo-Clustering-System-Optimization-HDBSCAN/blob/main/notebooks/10_Angular_SoftClustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# Notebook 10: Angular Distance + Soft Clustering\n",
    "\n",
    "## Eksperimen A â€” Angular Distance\n",
    "\n",
    "Di Notebook 6 (Distance Metric Compare), metrik `cosine` sudah diuji. Namun ada perbedaan kecil tapi penting antara `cosine distance` dan `angular distance`:\n",
    "\n",
    "```\n",
    "cosine_distance(u, v)  = 1 - dot(u,v) / (|u| Ã— |v|)\n",
    "angular_distance(u, v) = arccos(dot(u,v) / (|u| Ã— |v|)) / Ï€\n",
    "```\n",
    "\n",
    "**Mengapa angular lebih tepat untuk ArcFace embeddings?**\n",
    "\n",
    "ArcFace loss mengoptimasi *angular margin* di hypersphere. Artinya, jarak antar identitas dalam model ArcFace diukur dalam satuan sudut (radian), bukan selisih nilai cosine. Fungsi `arccos` memetakan cosine similarity ke sudut aktual, sehingga metrik yang digunakan HDBSCAN lebih sesuai dengan ruang geometris asli dimana embedding bekerja.\n",
    "\n",
    "Secara matematis, `angular_distance` adalah **metric yang valid** (memenuhi triangle inequality), sedangkan `cosine_distance` secara teknis tidak memenuhi triangle inequality untuk semua kasus.\n",
    "\n",
    "---\n",
    "\n",
    "## Eksperimen B â€” HDBSCAN Soft Clustering (Noise Assignment)\n",
    "\n",
    "HDBSCAN memiliki fitur **soft clustering** yang sering diabaikan. Setelah fitting selesai, poin yang dikategorikan sebagai noise dapat di-assign ke cluster terdekat berdasarkan fungsi membership probability.\n",
    "\n",
    "Mekanisme:\n",
    "1. Fit HDBSCAN dengan `prediction_data=True`\n",
    "2. Gunakan `hdbscan.approximate_predict()` untuk mendapatkan label dan strength untuk noise points\n",
    "3. Assign noise ke cluster jika strength â‰¥ threshold tertentu\n",
    "\n",
    "**Perbedaan dengan hard assignment:**\n",
    "- `approximate_predict` menggunakan condensed tree yang sudah dibangun, bukan nearest-neighbor sederhana\n",
    "- `strength` mencerminkan seberapa yakin assignment tersebut (0 = sangat tidak yakin, 1 = sangat yakin)\n",
    "- Kita bisa memilih threshold untuk mengontrol trade-off antara coverage dan kualitas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell1-md",
   "metadata": {},
   "source": [
    "## Cell 1 â€” Instalasi & Verifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hdbscan umap-learn -q\n",
    "\n",
    "import importlib\n",
    "\n",
    "def check_pkg(pkg, alias=None):\n",
    "    alias = alias or pkg\n",
    "    try:\n",
    "        mod = importlib.import_module(alias)\n",
    "        try:\n",
    "            ver = mod.__version__\n",
    "        except AttributeError:\n",
    "            from importlib.metadata import version\n",
    "            ver = version(pkg)\n",
    "        print(f\"   {pkg}: {ver} âœ…\")\n",
    "    except Exception as e:\n",
    "        print(f\"   {pkg}: âŒ {e}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"âœ… VERIFIKASI INSTALASI\")\n",
    "print(\"=\"*50)\n",
    "check_pkg('hdbscan')\n",
    "check_pkg('umap-learn', 'umap')\n",
    "check_pkg('scikit-learn', 'sklearn')\n",
    "check_pkg('numpy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell2-md",
   "metadata": {},
   "source": [
    "## Cell 2 â€” Imports & Konfigurasi\n",
    "\n",
    "Konfigurasi berisi:\n",
    "- **Baseline** dari Eksperimen 1.5 (Correlation, mcs=15, ms=70)\n",
    "- **Best UMAP config** dari Notebook 9 (diisi manual atau load dari pkl)\n",
    "- **Angular search space**: grid search ringkas untuk mcs dan ms\n",
    "- **Soft clustering thresholds**: nilai ambang batas strength yang akan dicoba (0.0, 0.1, 0.2, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "\n",
    "class Config:\n",
    "    EMBEDDINGS_PATH  = '/content/drive/MyDrive/OTW S.KOM/Embeddings/embeddings_data.pkl'\n",
    "    NOTEBOOK9_PATH   = '/content/drive/MyDrive/OTW S.KOM/Results/notebook9_umap_results.pkl'\n",
    "    RESULTS_DIR      = '/content/drive/MyDrive/OTW S.KOM/Results/'\n",
    "    PLOTS_DIR        = '/content/drive/MyDrive/OTW S.KOM/Results/Plots/'\n",
    "\n",
    "    BASELINE = {\n",
    "        'name'         : 'Correlation (mcs=15, ms=70)',\n",
    "        'silhouette'   : 0.4530,\n",
    "        'n_clusters'   : 54,\n",
    "        'noise_ratio'  : 0.438,\n",
    "        'coverage_rate': 0.562,\n",
    "    }\n",
    "\n",
    "    ANGULAR_PARAMS = {\n",
    "        'min_cluster_size': [10, 15, 20, 25],\n",
    "        'min_samples'     : [20, 30, 50, 70],\n",
    "        'method'          : ['eom'],\n",
    "    }\n",
    "\n",
    "    SOFT_THRESHOLDS = [0.0, 0.05, 0.1, 0.2, 0.3]\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… CONFIG LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ“Š BASELINE: {Config.BASELINE['name']}\")\n",
    "print(f\"   Coverage Rate: {Config.BASELINE['coverage_rate']:.1%}\")\n",
    "print(f\"   Silhouette   : {Config.BASELINE['silhouette']:.4f}\")\n",
    "print(f\"   Noise Ratio  : {Config.BASELINE['noise_ratio']:.1%}\")\n",
    "print(f\"\\nğŸ”§ Angular search: \"\n",
    "      f\"{len(Config.ANGULAR_PARAMS['min_cluster_size']) * len(Config.ANGULAR_PARAMS['min_samples'])} eksperimen\")\n",
    "print(f\"ğŸ”§ Soft clustering thresholds: {Config.SOFT_THRESHOLDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell3-md",
   "metadata": {},
   "source": [
    "## Cell 3 â€” Load Data\n",
    "\n",
    "Memuat embeddings asli dari disk. Angular distance dihitung dari embeddings yang dinormalisasi L2. Normalisasi membuat semua embedding berada di unit hypersphere sehingga dot product antar embedding ekuivalen dengan cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“ LOAD DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not os.path.exists('/content/drive/MyDrive'):\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"âœ… Google Drive sudah ter-mount\")\n",
    "\n",
    "os.makedirs(Config.RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(Config.PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "with open(Config.EMBEDDINGS_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "embeddings = np.array(data['embeddings']).astype('float32')\n",
    "metadata   = data['metadata']\n",
    "\n",
    "embeddings_l2 = normalize(embeddings, norm='l2')\n",
    "\n",
    "print(f\"\\nâœ… Data loaded!\")\n",
    "print(f\"   Embeddings     : {embeddings.shape}\")\n",
    "print(f\"   L2-normalized  : {embeddings_l2.shape}\")\n",
    "print(f\"   L2-norm check  : min={np.linalg.norm(embeddings_l2, axis=1).min():.6f}, \"\n",
    "      f\"max={np.linalg.norm(embeddings_l2, axis=1).max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell4-md",
   "metadata": {},
   "source": [
    "## Cell 4 â€” Fungsi Angular Distance Matrix (GPU)\n",
    "\n",
    "Angular distance dihitung sebagai:\n",
    "\n",
    "```\n",
    "angular_distance(u, v) = arccos(clip(dot(u_norm, v_norm), -1, 1)) / Ï€\n",
    "```\n",
    "\n",
    "Pembagian dengan Ï€ menormalisasi nilainya ke rentang [0, 1] sehingga konsisten dengan definisi metrik jarak standar.\n",
    "\n",
    "Komputasi menggunakan CuPy (GPU) karena dataset berukuran 12,715 Ã— 12,715 = ~1.2 GB untuk distance matrix `float32`. Operasi matrix multiplication pada GPU jauh lebih cepat.\n",
    "\n",
    "**Catatan:** Setelah normalisasi L2, `dot(u_norm, v_norm)` identik dengan cosine similarity. Langkah ekstra yang membedakan angular dari cosine adalah aplikasi `arccos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "\n",
    "def compute_angular_distance_matrix_gpu(embeddings_l2_normalized, verbose=True):\n",
    "    n = embeddings_l2_normalized.shape[0]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nâ³ Menghitung Angular distance matrix (GPU)...\")\n",
    "        print(f\"   Samples: {n:,}\")\n",
    "        print(f\"   Input sudah dinormalisasi L2: âœ…\")\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    emb_gpu  = cp.asarray(embeddings_l2_normalized, dtype=cp.float32)\n",
    "    dot_mat  = cp.dot(emb_gpu, emb_gpu.T)\n",
    "    cp.clip(dot_mat, -1.0, 1.0, out=dot_mat)\n",
    "    angle    = cp.arccos(dot_mat) / np.pi\n",
    "    cp.fill_diagonal(angle, 0.0)\n",
    "\n",
    "    dist_matrix = cp.asnumpy(angle)\n",
    "\n",
    "    del emb_gpu, dot_mat, angle\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "    elapsed = time.time() - t_start\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"   âœ… Selesai dalam {elapsed:.2f}s\")\n",
    "        print(f\"   Range: [{dist_matrix[dist_matrix > 0].min():.4f}, {dist_matrix.max():.4f}]\")\n",
    "        print(f\"   Memory: {dist_matrix.nbytes / 1024**2:.1f} MB\")\n",
    "\n",
    "    return dist_matrix\n",
    "\n",
    "\n",
    "print(\"âœ… compute_angular_distance_matrix_gpu() didefinisikan\")\n",
    "\n",
    "print(\"\\nğŸ§ª Verifikasi dengan 50 sample...\")\n",
    "test_emb  = normalize(np.random.randn(50, 512).astype('float32'), norm='l2')\n",
    "test_mat  = compute_angular_distance_matrix_gpu(test_emb, verbose=False)\n",
    "print(f\"   Symmetric       : {np.allclose(test_mat, test_mat.T)} âœ…\")\n",
    "print(f\"   Diagonal = 0    : {np.allclose(np.diag(test_mat), 0)} âœ…\")\n",
    "print(f\"   Range [0, 1]    : {test_mat.min():.4f} â€“ {test_mat.max():.4f} âœ…\")\n",
    "del test_emb, test_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell5-md",
   "metadata": {},
   "source": [
    "## Cell 5 â€” Hitung Angular Distance Matrix\n",
    "\n",
    "Langkah ini membutuhkan ~2â€“3 menit dan menggunakan ~1.2 GB GPU memory. Jika memory tidak cukup, kurangi `dtype` ke `float16` dengan konsekuensi sedikit kehilangan presisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "angular_dist_matrix = compute_angular_distance_matrix_gpu(embeddings_l2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell6-md",
   "metadata": {},
   "source": [
    "## Cell 6 â€” Fungsi Evaluasi & Grid Search Angular\n",
    "\n",
    "Silhouette Score untuk angular distance dihitung menggunakan `metric='precomputed'` agar benar-benar menggunakan angular distance matrix, bukan Euclidean. Ini konsisten dengan cara metrik dievaluasi di Notebook 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_angular_clustering(dist_matrix, embeddings_orig, labels):\n",
    "    n_total     = len(labels)\n",
    "    mask        = labels != -1\n",
    "    n_clustered = int(mask.sum())\n",
    "    n_clusters  = len(set(labels[mask])) if n_clustered > 0 else 0\n",
    "\n",
    "    result = {\n",
    "        'n_clusters'    : n_clusters,\n",
    "        'n_noise'       : int((labels == -1).sum()),\n",
    "        'noise_ratio'   : float((labels == -1).sum() / n_total),\n",
    "        'coverage_rate' : float(n_clustered / n_total),\n",
    "        'silhouette'    : None,\n",
    "        'dbi'           : None,\n",
    "    }\n",
    "\n",
    "    if n_clustered > 10 and n_clusters >= 2:\n",
    "        idx = np.where(mask)[0]\n",
    "        sub_dist = dist_matrix[np.ix_(idx, idx)]\n",
    "        result['silhouette'] = float(silhouette_score(sub_dist, labels[mask], metric='precomputed'))\n",
    "        result['dbi']        = float(davies_bouldin_score(embeddings_orig[mask], labels[mask]))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ”¬ EKSPERIMEN A: ANGULAR DISTANCE GRID SEARCH\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "angular_results = []\n",
    "\n",
    "mcs_list = Config.ANGULAR_PARAMS['min_cluster_size']\n",
    "ms_list  = Config.ANGULAR_PARAMS['min_samples']\n",
    "\n",
    "print(f\"\\n{'mcs':>4} {'ms':>4} | {'clusters':>8} {'coverage':>9} {'noise':>7} {'silhouette':>11}\")\n",
    "print(f\"{'â”€'*4} {'â”€'*4} | {'â”€'*8} {'â”€'*9} {'â”€'*7} {'â”€'*11}\")\n",
    "\n",
    "for mcs in mcs_list:\n",
    "    for ms in ms_list:\n",
    "        t_start   = time.time()\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size         = mcs,\n",
    "            min_samples              = ms,\n",
    "            cluster_selection_method = 'eom',\n",
    "            metric                   = 'precomputed',\n",
    "        )\n",
    "        labels  = clusterer.fit_predict(angular_dist_matrix)\n",
    "        elapsed = time.time() - t_start\n",
    "        metrics = evaluate_angular_clustering(angular_dist_matrix, embeddings, labels)\n",
    "\n",
    "        angular_results.append({\n",
    "            'min_cluster_size': mcs,\n",
    "            'min_samples'     : ms,\n",
    "            'labels'          : labels.copy(),\n",
    "            'time'            : elapsed,\n",
    "            **metrics,\n",
    "        })\n",
    "\n",
    "        sil_str = f\"{metrics['silhouette']:.4f}\" if metrics['silhouette'] else \"   N/A\"\n",
    "        print(f\"{mcs:>4} {ms:>4} | \"\n",
    "              f\"{metrics['n_clusters']:>8} \"\n",
    "              f\"{metrics['coverage_rate']:>9.1%} \"\n",
    "              f\"{metrics['noise_ratio']:>7.1%} \"\n",
    "              f\"{sil_str:>11}\")\n",
    "\n",
    "df_ang  = pd.DataFrame([{k: v for k, v in r.items() if k != 'labels'} for r in angular_results])\n",
    "df_valid_ang = df_ang.dropna(subset=['silhouette'])\n",
    "\n",
    "best_ang_cov = df_valid_ang.loc[df_valid_ang['coverage_rate'].idxmax()]\n",
    "best_ang_sil = df_valid_ang.loc[df_valid_ang['silhouette'].idxmax()]\n",
    "\n",
    "print(f\"\\nğŸ† Best by Coverage : mcs={int(best_ang_cov['min_cluster_size'])}, ms={int(best_ang_cov['min_samples'])}  \"\n",
    "      f\"â†’ coverage={best_ang_cov['coverage_rate']:.1%}, sil={best_ang_cov['silhouette']:.4f}\")\n",
    "print(f\"ğŸ† Best by Silhouette: mcs={int(best_ang_sil['min_cluster_size'])}, ms={int(best_ang_sil['min_samples'])}  \"\n",
    "      f\"â†’ coverage={best_ang_sil['coverage_rate']:.1%}, sil={best_ang_sil['silhouette']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell7-md",
   "metadata": {},
   "source": [
    "## Cell 7 â€” Eksperimen B: Soft Clustering (Noise Assignment)\n",
    "\n",
    "Konfigurasi terbaik dari Angular grid search digunakan ulang, kali ini dengan `prediction_data=True`. Setelah fitting, noise points di-assign ke cluster menggunakan `hdbscan.approximate_predict()`.\n",
    "\n",
    "Setiap threshold strength menghasilkan trade-off berbeda:\n",
    "- **Threshold = 0.0** â€” assign semua noise ke cluster terdekat tanpa syarat (coverage maksimum, tapi banyak assignment tidak pasti)\n",
    "- **Threshold = 0.1** â€” hanya assign noise yang strength-nya â‰¥ 0.1 (lebih conservative)\n",
    "- **Threshold = 0.3** â€” hanya assign noise yang strength-nya â‰¥ 0.3 (paling selektif)\n",
    "\n",
    "Kita evaluasi setiap threshold untuk melihat hubungan antara coverage gain dan kualitas cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ğŸ”¬ EKSPERIMEN B: SOFT CLUSTERING â€” NOISE ASSIGNMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_mcs = int(best_ang_cov['min_cluster_size'])\n",
    "best_ms  = int(best_ang_cov['min_samples'])\n",
    "\n",
    "print(f\"\\nâš™ï¸ Menggunakan Angular best coverage: mcs={best_mcs}, ms={best_ms}\")\n",
    "print(f\"   (precomputed Angular distance matrix)\")\n",
    "\n",
    "clusterer_soft = hdbscan.HDBSCAN(\n",
    "    min_cluster_size         = best_mcs,\n",
    "    min_samples              = best_ms,\n",
    "    cluster_selection_method = 'eom',\n",
    "    metric                   = 'precomputed',\n",
    "    prediction_data          = True,\n",
    ")\n",
    "\n",
    "print(\"\\nâ³ Fitting HDBSCAN dengan prediction_data=True...\")\n",
    "t_fit          = time.time()\n",
    "labels_base    = clusterer_soft.fit_predict(angular_dist_matrix)\n",
    "fit_time       = time.time() - t_fit\n",
    "print(f\"   âœ… Selesai ({fit_time:.2f}s)\")\n",
    "\n",
    "noise_mask        = labels_base == -1\n",
    "noise_indices     = np.where(noise_mask)[0]\n",
    "n_noise_original  = noise_mask.sum()\n",
    "\n",
    "print(f\"\\nğŸ“Š Baseline (Angular, hard clustering):\")\n",
    "base_metrics = evaluate_angular_clustering(angular_dist_matrix, embeddings, labels_base)\n",
    "print(f\"   Coverage: {base_metrics['coverage_rate']:.1%}  |  Noise: {n_noise_original:,}  |  Silhouette: {base_metrics['silhouette']:.4f}\")\n",
    "\n",
    "print(f\"\\nâ³ Menjalankan approximate_predict pada {n_noise_original:,} noise points...\")\n",
    "\n",
    "noise_dist_submatrix  = angular_dist_matrix[np.ix_(noise_indices, np.arange(len(labels_base)))]\n",
    "\n",
    "soft_results = []\n",
    "\n",
    "try:\n",
    "    soft_labels, soft_strengths = hdbscan.approximate_predict(\n",
    "        clusterer_soft,\n",
    "        noise_dist_submatrix,\n",
    "    )\n",
    "    predict_available = True\n",
    "    print(f\"   âœ… approximate_predict selesai\")\n",
    "    print(f\"   Strength range: [{soft_strengths.min():.4f}, {soft_strengths.max():.4f}]\")\n",
    "    print(f\"   Strength mean : {soft_strengths.mean():.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ approximate_predict tidak bisa dijalankan dengan precomputed matrix: {e}\")\n",
    "    print(f\"   Menggunakan fallback: k-NN cluster assignment\")\n",
    "    predict_available = False\n",
    "\n",
    "print(f\"\\n{'threshold':>10} | {'assigned':>9} {'coverage':>9} {'still_noise':>12}\")\n",
    "print(f\"{'â”€'*10} | {'â”€'*9} {'â”€'*9} {'â”€'*12}\")\n",
    "\n",
    "if predict_available:\n",
    "    for threshold in Config.SOFT_THRESHOLDS:\n",
    "        labels_new = labels_base.copy()\n",
    "\n",
    "        for i, (noise_idx, pred_lbl, strength) in enumerate(\n",
    "            zip(noise_indices, soft_labels, soft_strengths)\n",
    "        ):\n",
    "            if pred_lbl != -1 and strength >= threshold:\n",
    "                labels_new[noise_idx] = pred_lbl\n",
    "\n",
    "        n_assigned    = int((labels_new != -1).sum()) - int((labels_base != -1).sum())\n",
    "        n_still_noise = int((labels_new == -1).sum())\n",
    "        coverage_new  = float((labels_new != -1).sum() / len(labels_new))\n",
    "\n",
    "        metrics_new = evaluate_angular_clustering(angular_dist_matrix, embeddings, labels_new)\n",
    "\n",
    "        soft_results.append({\n",
    "            'threshold'    : threshold,\n",
    "            'n_assigned'   : n_assigned,\n",
    "            'n_still_noise': n_still_noise,\n",
    "            'labels'       : labels_new.copy(),\n",
    "            **metrics_new,\n",
    "        })\n",
    "\n",
    "        print(f\"{threshold:>10.2f} | {n_assigned:>9,} {coverage_new:>9.1%} {n_still_noise:>12,}\")\n",
    "\n",
    "else:\n",
    "    print(\"   Menggunakan kNN fallback...\")\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    clustered_idx    = np.where(~noise_mask)[0]\n",
    "    clustered_labels = labels_base[clustered_idx]\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "    knn.fit(embeddings[clustered_idx], clustered_labels)\n",
    "\n",
    "    proba        = knn.predict_proba(embeddings[noise_indices])\n",
    "    knn_labels   = knn.predict(embeddings[noise_indices])\n",
    "    knn_strengths = proba.max(axis=1)\n",
    "\n",
    "    for threshold in Config.SOFT_THRESHOLDS:\n",
    "        labels_new = labels_base.copy()\n",
    "        for i, noise_idx in enumerate(noise_indices):\n",
    "            if knn_strengths[i] >= threshold:\n",
    "                labels_new[noise_idx] = knn_labels[i]\n",
    "\n",
    "        n_assigned    = int((labels_new != -1).sum()) - int((labels_base != -1).sum())\n",
    "        n_still_noise = int((labels_new == -1).sum())\n",
    "        coverage_new  = float((labels_new != -1).sum() / len(labels_new))\n",
    "\n",
    "        metrics_new = evaluate_angular_clustering(angular_dist_matrix, embeddings, labels_new)\n",
    "\n",
    "        soft_results.append({\n",
    "            'threshold'    : threshold,\n",
    "            'n_assigned'   : n_assigned,\n",
    "            'n_still_noise': n_still_noise,\n",
    "            'labels'       : labels_new.copy(),\n",
    "            **metrics_new,\n",
    "        })\n",
    "\n",
    "        print(f\"{threshold:>10.2f} | {n_assigned:>9,} {coverage_new:>9.1%} {n_still_noise:>12,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell8-md",
   "metadata": {},
   "source": [
    "## Cell 8 â€” Visualisasi Perbandingan\n",
    "\n",
    "Plot ini menampilkan:\n",
    "1. **Coverage Rate vs Threshold** â€” bagaimana coverage berubah seiring threshold yang dinaikkan\n",
    "2. **Silhouette vs Threshold** â€” apakah kualitas cluster terjaga setelah noise assignment\n",
    "3. **Perbandingan metode** â€” Angular vs Baseline Correlation, sebelum dan sesudah soft clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soft = pd.DataFrame([\n",
    "    {k: v for k, v in r.items() if k != 'labels'} for r in soft_results\n",
    "])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "ax1, ax2, ax3 = axes\n",
    "\n",
    "ax1.plot(df_soft['threshold'], df_soft['coverage_rate'] * 100, 'o-', color='darkorange', linewidth=2, markersize=8)\n",
    "ax1.axhline(Config.BASELINE['coverage_rate'] * 100, color='steelblue', linestyle='--', linewidth=1.5,\n",
    "            label=f\"Baseline: {Config.BASELINE['coverage_rate']:.1%}\")\n",
    "ax1.axhline(base_metrics['coverage_rate'] * 100, color='red', linestyle=':', linewidth=1.5,\n",
    "            label=f\"Angular hard: {base_metrics['coverage_rate']:.1%}\")\n",
    "ax1.set_xlabel('Strength Threshold')\n",
    "ax1.set_ylabel('Coverage Rate (%)')\n",
    "ax1.set_title('Coverage Rate vs Threshold\\n(Soft Clustering)', fontweight='bold')\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(df_soft['threshold'], df_soft['silhouette'], 's-', color='forestgreen', linewidth=2, markersize=8)\n",
    "ax2.axhline(Config.BASELINE['silhouette'], color='steelblue', linestyle='--', linewidth=1.5,\n",
    "            label=f\"Baseline: {Config.BASELINE['silhouette']:.4f}\")\n",
    "ax2.axhline(base_metrics['silhouette'], color='red', linestyle=':', linewidth=1.5,\n",
    "            label=f\"Angular hard: {base_metrics['silhouette']:.4f}\")\n",
    "ax2.set_xlabel('Strength Threshold')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Score vs Threshold\\n(Soft Clustering)', fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "methods = ['Baseline\\n(Correlation)', 'Angular\\n(Hard)', 'Angular+Soft\\n(thr=0.0)', 'Angular+Soft\\n(thr=0.1)']\n",
    "cov_vals = [\n",
    "    Config.BASELINE['coverage_rate'] * 100,\n",
    "    base_metrics['coverage_rate'] * 100,\n",
    "]\n",
    "sil_vals_bar = [\n",
    "    Config.BASELINE['silhouette'],\n",
    "    base_metrics['silhouette'],\n",
    "]\n",
    "\n",
    "thr_map = {r['threshold']: r for r in soft_results}\n",
    "for thr in [0.0, 0.1]:\n",
    "    if thr in thr_map:\n",
    "        cov_vals.append(thr_map[thr]['coverage_rate'] * 100)\n",
    "        sil_vals_bar.append(thr_map[thr]['silhouette'])\n",
    "\n",
    "colors_bar = ['steelblue', 'red', 'darkorange', 'gold']\n",
    "x = np.arange(len(methods[:len(cov_vals)]))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax3.bar(x - width/2, cov_vals, width, label='Coverage Rate (%)', color=[c + '99' for c in ['#1f77b4','#d62728','#ff7f0e','#ffdd57']], edgecolor='black')\n",
    "ax3_r = ax3.twinx()\n",
    "bars2 = ax3_r.bar(x + width/2, sil_vals_bar, width, label='Silhouette', color=colors_bar[:len(cov_vals)], edgecolor='black', alpha=0.7)\n",
    "\n",
    "ax3.set_ylabel('Coverage Rate (%)')\n",
    "ax3_r.set_ylabel('Silhouette Score')\n",
    "ax3.set_title('Perbandingan Metode\\n(Coverage & Silhouette)', fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(methods[:len(cov_vals)], fontsize=9)\n",
    "ax3.set_ylim(0, 110)\n",
    "ax3_r.set_ylim(0, max(sil_vals_bar) * 1.4 if sil_vals_bar else 1)\n",
    "\n",
    "for bar, val in zip(bars1, cov_vals):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{val:.1f}%', ha='center', fontsize=8)\n",
    "for bar, val in zip(bars2, sil_vals_bar):\n",
    "    if val:\n",
    "        ax3_r.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, f'{val:.3f}', ha='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = f\"{Config.PLOTS_DIR}notebook10_angular_softclustering.png\"\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"ğŸ’¾ Plot disimpan: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell9-md",
   "metadata": {},
   "source": [
    "## Cell 9 â€” Summary & Simpan Hasil\n",
    "\n",
    "Memilih konfigurasi soft clustering terbaik berdasarkan trade-off Coverage Rate vs Silhouette, lalu menyimpan semua hasil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_soft = max(\n",
    "    [r for r in soft_results if r.get('silhouette')],\n",
    "    key=lambda r: r['coverage_rate']\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“‹ NOTEBOOK 10 â€” SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  EKSPERIMEN A: ANGULAR DISTANCE                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Metrik                â”‚ Baseline        â”‚ Angular (Best Coverage)       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Coverage Rate [â†‘]     â”‚ {Config.BASELINE['coverage_rate']:.1%}           â”‚ {best_ang_cov['coverage_rate']:.1%}                        â”‚\n",
    "â”‚ Noise Ratio   [â†“]     â”‚ {Config.BASELINE['noise_ratio']:.1%}           â”‚ {best_ang_cov['noise_ratio']:.1%}                        â”‚\n",
    "â”‚ Silhouette    [â†‘]     â”‚ {Config.BASELINE['silhouette']:.4f}          â”‚ {best_ang_cov['silhouette']:.4f}                       â”‚\n",
    "â”‚ Clusters              â”‚ {Config.BASELINE['n_clusters']}              â”‚ {int(best_ang_cov['n_clusters'])}                          â”‚\n",
    "â”‚ mcs / ms              â”‚ 15 / 70         â”‚ {int(best_ang_cov['min_cluster_size'])} / {int(best_ang_cov['min_samples'])}                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  EKSPERIMEN B: SOFT CLUSTERING                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Metrik                â”‚ Angular Hard    â”‚ Angular + Soft (thr=0.0)      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Coverage Rate [â†‘]     â”‚ {base_metrics['coverage_rate']:.1%}           â”‚ {best_soft['coverage_rate']:.1%}                        â”‚\n",
    "â”‚ Noise Assigned        â”‚ 0               â”‚ {best_soft['n_assigned']:,}                      â”‚\n",
    "â”‚ Silhouette    [â†‘]     â”‚ {base_metrics['silhouette']:.4f if base_metrics['silhouette'] else 'N/A':7}          â”‚ {best_soft['silhouette']:.4f if best_soft.get('silhouette') else 'N/A':7}                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "save_data = {\n",
    "    'timestamp'           : datetime.now().isoformat(),\n",
    "    'notebook'            : 'Notebook 10: Angular Distance + Soft Clustering',\n",
    "    'baseline'            : Config.BASELINE,\n",
    "    'angular_results'     : [{k: v for k, v in r.items() if k != 'labels'} for r in angular_results],\n",
    "    'best_angular_coverage': {\n",
    "        'min_cluster_size': int(best_ang_cov['min_cluster_size']),\n",
    "        'min_samples'     : int(best_ang_cov['min_samples']),\n",
    "        'n_clusters'      : int(best_ang_cov['n_clusters']),\n",
    "        'coverage_rate'   : float(best_ang_cov['coverage_rate']),\n",
    "        'noise_ratio'     : float(best_ang_cov['noise_ratio']),\n",
    "        'silhouette'      : float(best_ang_cov['silhouette']),\n",
    "        'labels'          : angular_results[df_valid_ang['coverage_rate'].argmax()]['labels'],\n",
    "    },\n",
    "    'soft_clustering_results': [{k: v for k, v in r.items() if k != 'labels'} for r in soft_results],\n",
    "    'best_soft_labels'    : best_soft['labels'],\n",
    "    'angular_dist_matrix' : angular_dist_matrix,\n",
    "}\n",
    "\n",
    "pkl_path = f\"{Config.RESULTS_DIR}notebook10_angular_soft_results.pkl\"\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump(save_data, f)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Disimpan: {pkl_path}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ NEXT: Notebook 11 â€” Analisis Komposisi Noise\")\n",
    "print(\"=\"*70)"
   ]
  }
 ]
}
