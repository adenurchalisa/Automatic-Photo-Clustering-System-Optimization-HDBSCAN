{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adenurchalisa/Automatic-Photo-Clustering-System-Optimization-HDBSCAN/blob/main/notebooks/11_Noise_Composition_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# Notebook 11: Analisis Komposisi Noise\n",
    "\n",
    "## Mengapa Notebook Ini Penting?\n",
    "\n",
    "Sebelum masuk ke CGA (Cluster Generation/Augmentation) di Eksperimen 2, kita perlu memahami **mengapa** suatu poin menjadi noise. Ada dua kemungkinan yang memiliki implikasi sangat berbeda:\n",
    "\n",
    "### Tipe 1: Genuine Singleton (Few-Shot Person)\n",
    "Orang yang hanya muncul satu atau beberapa kali dalam seluruh dataset. Embedding mereka valid, namun jumlahnya terlalu sedikit untuk membentuk cluster dengan `min_cluster_size` yang ditentukan. **Ini adalah justifikasi utama CGA** ‚Äî synthetic augmentation dapat \"memperbanyak\" embedding orang ini agar mencapai threshold minimum cluster.\n",
    "\n",
    "### Tipe 2: Outlier Kualitas Rendah (Low-Quality Detection)\n",
    "Embedding dari foto yang blur, pose ekstrem, oklusu wajah berat, atau pencahayaan buruk. HDBSCAN membuang ini karena berada di region berdensitas sangat rendah di embedding space ‚Äî bukan karena orang tersebut langka, tapi karena embedding-nya tidak representatif.\n",
    "\n",
    "**Jika mayoritas noise adalah Tipe 2, CGA tidak akan efektif** ‚Äî menambah synthetic embedding dari sumber yang sudah berkualitas buruk hanya akan memperburuk cluster. Sebaliknya, jika mayoritas adalah Tipe 1, CGA memiliki justifikasi yang kuat.\n",
    "\n",
    "## Metodologi Analisis\n",
    "\n",
    "1. **Frekuensi per identitas dalam noise** ‚Äî apakah noise terdistribusi ke banyak orang (singleton) atau terkonsentrasi?\n",
    "2. **Per-image noise distribution** ‚Äî apakah satu foto memiliki banyak noise? Ini indikasi kualitas foto buruk.\n",
    "3. **Embedding quality proxy** ‚Äî menghitung L2-norm dan jarak ke nearest clustered neighbor sebagai proxy kualitas\n",
    "4. **Iterative re-clustering noise** ‚Äî pisahkan noise points lalu cluster ulang dengan `min_cluster_size` yang lebih kecil\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell1-md",
   "metadata": {},
   "source": [
    "## Cell 1 ‚Äî Instalasi & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hdbscan -q\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"‚úÖ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell2-md",
   "metadata": {},
   "source": [
    "## Cell 2 ‚Äî Konfigurasi\n",
    "\n",
    "Notebook ini memuat hasil dari Eksperimen 1.5 (Correlation best config) yang menjadi baseline utama analisis noise. Jika hasil dari Notebook 9 atau 10 lebih baik, labels dari sana bisa disubstitusi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    EMBEDDINGS_PATH = '/content/drive/MyDrive/OTW S.KOM/Embeddings/embeddings_data.pkl'\n",
    "    RESULTS_DIR     = '/content/drive/MyDrive/OTW S.KOM/Results/'\n",
    "    PLOTS_DIR       = '/content/drive/MyDrive/OTW S.KOM/Results/Plots/'\n",
    "\n",
    "    BASELINE_PARAMS = {\n",
    "        'metric'           : 'correlation',\n",
    "        'min_cluster_size' : 15,\n",
    "        'min_samples'      : 70,\n",
    "        'method'           : 'eom',\n",
    "    }\n",
    "\n",
    "    REFIT_BASELINE = True\n",
    "\n",
    "    RECLUSTERING_MCS_LIST = [2, 3, 4, 5, 7, 10]\n",
    "    RECLUSTERING_MS_LIST  = [1, 2, 3, 5]\n",
    "\n",
    "    SINGLETON_THRESHOLD = 3\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ CONFIG LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n  Baseline: Correlation, mcs={Config.BASELINE_PARAMS['min_cluster_size']}, \"\n",
    "      f\"ms={Config.BASELINE_PARAMS['min_samples']}\")\n",
    "print(f\"  Singleton threshold: ‚â§ {Config.SINGLETON_THRESHOLD} kemunculan\")\n",
    "print(f\"  Re-clustering MCS grid: {Config.RECLUSTERING_MCS_LIST}\")\n",
    "print(f\"  Re-clustering MS grid : {Config.RECLUSTERING_MS_LIST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell3-md",
   "metadata": {},
   "source": [
    "## Cell 3 ‚Äî Load Data & Reproduce Baseline\n",
    "\n",
    "Baseline labels (Correlation, mcs=15, ms=70) direproduksi ulang menggunakan `metric='correlation'` pada raw embeddings. Ini diperlukan karena label aktual perlu tersedia di memori untuk analisis noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìÅ LOAD DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not os.path.exists('/content/drive/MyDrive'):\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"‚úÖ Google Drive sudah ter-mount\")\n",
    "\n",
    "os.makedirs(Config.RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(Config.PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "with open(Config.EMBEDDINGS_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "embeddings = np.array(data['embeddings']).astype('float32')\n",
    "metadata   = data['metadata']\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded\")\n",
    "print(f\"   Embeddings : {embeddings.shape}\")\n",
    "print(f\"   Metadata   : {len(metadata)} entries\")\n",
    "\n",
    "meta_keys = list(metadata[0].keys()) if metadata else []\n",
    "print(f\"   Meta keys  : {meta_keys}\")\n",
    "\n",
    "print(f\"\\n‚è≥ Reproducing baseline clustering (Correlation, mcs=15, ms=70)...\")\n",
    "t_start = time.time()\n",
    "\n",
    "clusterer_baseline = hdbscan.HDBSCAN(\n",
    "    min_cluster_size         = Config.BASELINE_PARAMS['min_cluster_size'],\n",
    "    min_samples              = Config.BASELINE_PARAMS['min_samples'],\n",
    "    cluster_selection_method = Config.BASELINE_PARAMS['method'],\n",
    "    metric                   = Config.BASELINE_PARAMS['metric'],\n",
    ")\n",
    "labels_baseline = clusterer_baseline.fit_predict(embeddings)\n",
    "elapsed = time.time() - t_start\n",
    "\n",
    "n_clusters_base = len(set(labels_baseline)) - 1\n",
    "n_noise_base    = int((labels_baseline == -1).sum())\n",
    "coverage_base   = float((labels_baseline != -1).sum() / len(labels_baseline))\n",
    "\n",
    "print(f\"   ‚úÖ Selesai ({elapsed:.2f}s)\")\n",
    "print(f\"   Clusters     : {n_clusters_base}\")\n",
    "print(f\"   Noise        : {n_noise_base:,} ({1-coverage_base:.1%})\")\n",
    "print(f\"   Coverage Rate: {coverage_base:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell4-md",
   "metadata": {},
   "source": [
    "## Cell 4 ‚Äî Analisis Distribusi Noise per Gambar\n",
    "\n",
    "Pertanyaan: apakah noise points terkonsentrasi di gambar-gambar tertentu?\n",
    "\n",
    "Jika ya ‚Üí kemungkinan besar disebabkan oleh **kualitas foto buruk** (blur, pencahayaan, dll.), bukan oleh few-shot person.\n",
    "\n",
    "Jika tersebar merata ‚Üí kemungkinan besar adalah **genuine singletons** dari banyak individu berbeda.\n",
    "\n",
    "Key metadata yang dianalisis:\n",
    "- `image_filename` atau `image_path` ‚Äî untuk mengidentifikasi gambar sumber\n",
    "- `face_id` atau `det_score` ‚Äî detection confidence score dari InsightFace (jika tersedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_mask    = labels_baseline == -1\n",
    "noise_indices = np.where(noise_mask)[0]\n",
    "noise_meta    = [metadata[i] for i in noise_indices]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä ANALISIS 1: DISTRIBUSI NOISE PER GAMBAR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "img_key = None\n",
    "for candidate in ['image_filename', 'image_path', 'img_path', 'filename', 'path']:\n",
    "    if noise_meta and candidate in noise_meta[0]:\n",
    "        img_key = candidate\n",
    "        break\n",
    "\n",
    "if img_key:\n",
    "    noise_per_img    = Counter([m[img_key] for m in noise_meta])\n",
    "    all_per_img      = Counter([metadata[i][img_key] for i in range(len(metadata))])\n",
    "    noise_ratio_img  = {img: noise_per_img.get(img, 0) / total\n",
    "                        for img, total in all_per_img.items()}\n",
    "\n",
    "    df_img = pd.DataFrame([\n",
    "        {'image': k, 'total_faces': all_per_img[k], 'noise_faces': noise_per_img.get(k, 0),\n",
    "         'noise_ratio': noise_ratio_img[k]}\n",
    "        for k in all_per_img\n",
    "    ]).sort_values('noise_ratio', ascending=False)\n",
    "\n",
    "    print(f\"\\n  Key metadata gambar: '{img_key}'\")\n",
    "    print(f\"  Total gambar unik: {len(all_per_img):,}\")\n",
    "    print(f\"  Gambar dengan noise_ratio = 100%: {(df_img['noise_ratio'] == 1.0).sum():,}\")\n",
    "    print(f\"  Gambar dengan noise_ratio = 0%  : {(df_img['noise_ratio'] == 0.0).sum():,}\")\n",
    "    print(f\"  Rata-rata noise per gambar: {df_img['noise_ratio'].mean():.1%}\")\n",
    "    print(f\"\\n  Top 10 gambar dengan noise ratio tertinggi:\")\n",
    "    print(df_img.head(10).to_string(index=False))\n",
    "\n",
    "    high_noise_imgs = df_img[df_img['noise_ratio'] > 0.8]\n",
    "    print(f\"\\n  Gambar dengan >80% wajah menjadi noise: {len(high_noise_imgs):,} gambar\")\n",
    "    print(f\"  Ini berisi {high_noise_imgs['noise_faces'].sum():,} noise points \"\n",
    "          f\"({high_noise_imgs['noise_faces'].sum() / n_noise_base:.1%} dari total noise)\")\n",
    "\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è Metadata key untuk gambar tidak ditemukan.\")\n",
    "    print(f\"  Keys yang tersedia: {list(noise_meta[0].keys()) if noise_meta else 'kosong'}\")\n",
    "    print(\"  Analisis ini dilewati. Lanjut ke analisis embedding quality.\")\n",
    "    df_img = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell5-md",
   "metadata": {},
   "source": [
    "## Cell 5 ‚Äî Analisis Kualitas Embedding Noise\n",
    "\n",
    "Dua proxy kualitas yang digunakan:\n",
    "\n",
    "1. **L2-norm embedding** ‚Äî InsightFace menghasilkan embedding dengan L2-norm yang bervariasi. Embedding dari foto berkualitas rendah (blur, oklusu) cenderung memiliki L2-norm yang ekstrem (sangat kecil atau sangat besar) dibanding embedding berkualitas baik.\n",
    "\n",
    "2. **Jarak ke nearest clustered neighbor (DNCN)** ‚Äî seberapa jauh noise point dari cluster terdekat. Noise dengan DNCN sangat besar kemungkinan adalah outlier sejati. Noise dengan DNCN kecil berarti dia hampir masuk cluster, bisa jadi genuine singleton yang gagal karena threshold `min_cluster_size`.\n",
    "\n",
    "3. **Detection score** ‚Äî InsightFace menyimpan confidence score deteksi wajah (`det_score`). Wajah dengan `det_score` rendah lebih mungkin menjadi noise karena embedding-nya tidak representatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìä ANALISIS 2: KUALITAS EMBEDDING NOISE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "noise_emb      = embeddings[noise_mask]\n",
    "clustered_emb  = embeddings[~noise_mask]\n",
    "\n",
    "noise_norms    = np.linalg.norm(noise_emb, axis=1)\n",
    "cluster_norms  = np.linalg.norm(clustered_emb, axis=1)\n",
    "\n",
    "print(f\"\\n  L2-Norm Comparison:\")\n",
    "print(f\"  {'Metric':<20} {'Noise':>12} {'Clustered':>12} {'Diff':>10}\")\n",
    "print(f\"  {'‚îÄ'*20} {'‚îÄ'*12} {'‚îÄ'*12} {'‚îÄ'*10}\")\n",
    "for metric_name, fn in [('Mean', np.mean), ('Std', np.std), ('Min', np.min), ('Max', np.max)]:\n",
    "    n_val = fn(noise_norms)\n",
    "    c_val = fn(cluster_norms)\n",
    "    print(f\"  {metric_name:<20} {n_val:>12.4f} {c_val:>12.4f} {n_val-c_val:>+10.4f}\")\n",
    "\n",
    "print(f\"\\n‚è≥ Menghitung jarak ke nearest clustered neighbor...\")\n",
    "print(f\"   (menggunakan 1-NN di ruang embedding asli, Euclidean)\")\n",
    "\n",
    "sample_size = min(3000, len(noise_emb))\n",
    "noise_sample_idx = np.random.choice(len(noise_emb), sample_size, replace=False)\n",
    "noise_sample     = noise_emb[noise_sample_idx]\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=1, metric='euclidean', n_jobs=-1)\n",
    "nn.fit(clustered_emb)\n",
    "dists, _ = nn.kneighbors(noise_sample)\n",
    "dncn     = dists.flatten()\n",
    "\n",
    "print(f\"   ‚úÖ Selesai (sample: {sample_size:,} noise points)\")\n",
    "print(f\"\\n  Jarak ke nearest clustered neighbor (DNCN):\")\n",
    "print(f\"  Mean   : {dncn.mean():.4f}\")\n",
    "print(f\"  Median : {np.median(dncn):.4f}\")\n",
    "print(f\"  Std    : {dncn.std():.4f}\")\n",
    "print(f\"  Min    : {dncn.min():.4f}\")\n",
    "print(f\"  Max    : {dncn.max():.4f}\")\n",
    "\n",
    "pct_close_noise = (dncn < np.percentile(dncn, 25)).mean()\n",
    "print(f\"\\n  Noise points di bawah Q25 DNCN (hampir masuk cluster): {pct_close_noise:.1%}\")\n",
    "print(f\"  Ini adalah kandidat kuat untuk CGA (mereka genuine singletons, bukan outlier kualitas buruk)\")\n",
    "\n",
    "det_key = None\n",
    "for candidate in ['det_score', 'detection_score', 'score', 'confidence']:\n",
    "    if noise_meta and candidate in noise_meta[0]:\n",
    "        det_key = candidate\n",
    "        break\n",
    "\n",
    "if det_key:\n",
    "    noise_det_scores = np.array([m[det_key] for m in noise_meta])\n",
    "    cluster_det_scores = np.array([metadata[i][det_key] for i in range(len(metadata)) if not noise_mask[i]])\n",
    "    print(f\"\\n  Detection Score ('{det_key}'):\")\n",
    "    print(f\"  Noise mean    : {noise_det_scores.mean():.4f}\")\n",
    "    print(f\"  Clustered mean: {cluster_det_scores.mean():.4f}\")\n",
    "    print(f\"  Noise < 0.5   : {(noise_det_scores < 0.5).mean():.1%} (indikasi kualitas rendah)\")\n",
    "else:\n",
    "    print(f\"\\n  ‚ÑπÔ∏è Detection score tidak ditemukan di metadata.\")\n",
    "    print(f\"  Keys tersedia: {list(noise_meta[0].keys()) if noise_meta else 'kosong'}\")\n",
    "    det_key = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell6-md",
   "metadata": {},
   "source": [
    "## Cell 6 ‚Äî Klasifikasi Noise: Singleton vs Low-Quality\n",
    "\n",
    "Mengklasifikasikan noise points berdasarkan proximity ke cluster yang sudah ada:\n",
    "\n",
    "- **Probable Singleton (CGA Candidate)**: DNCN < Q25 ‚Äî dekat dengan cluster yang ada, kemungkinan besar adalah orang yang sama dengan anggota cluster tersebut tapi jumlahnya tidak cukup untuk membentuk cluster baru\n",
    "- **Borderline**: DNCN Q25‚ÄìQ75 ‚Äî tidak dapat ditentukan dengan jelas  \n",
    "- **Probable Outlier**: DNCN > Q75 ‚Äî jauh dari semua cluster yang ada, kemungkinan kualitas embedding buruk\n",
    "\n",
    "Klasifikasi ini langsung menjawab pertanyaan: **seberapa banyak noise yang bisa dibantu oleh CGA?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìä ANALISIS 3: KLASIFIKASI NOISE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "nn_full = NearestNeighbors(n_neighbors=1, metric='euclidean', n_jobs=-1)\n",
    "nn_full.fit(clustered_emb)\n",
    "\n",
    "batch_size = 1000\n",
    "all_dncn   = []\n",
    "\n",
    "print(f\"\\n‚è≥ Menghitung DNCN untuk semua {n_noise_base:,} noise points...\")\n",
    "for start in range(0, len(noise_emb), batch_size):\n",
    "    end  = min(start + batch_size, len(noise_emb))\n",
    "    d, _ = nn_full.kneighbors(noise_emb[start:end])\n",
    "    all_dncn.extend(d.flatten().tolist())\n",
    "\n",
    "dncn_all = np.array(all_dncn)\n",
    "print(f\"   ‚úÖ Selesai\")\n",
    "\n",
    "q25 = np.percentile(dncn_all, 25)\n",
    "q75 = np.percentile(dncn_all, 75)\n",
    "\n",
    "singleton_mask   = dncn_all < q25\n",
    "borderline_mask  = (dncn_all >= q25) & (dncn_all <= q75)\n",
    "outlier_mask     = dncn_all > q75\n",
    "\n",
    "n_singleton  = singleton_mask.sum()\n",
    "n_borderline = borderline_mask.sum()\n",
    "n_outlier    = outlier_mask.sum()\n",
    "\n",
    "print(f\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    KLASIFIKASI NOISE POINTS                           ‚îÇ\n",
    "‚îÇ                    (berdasarkan DNCN quartile)                        ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ Kategori                 ‚îÇ    Jumlah    ‚îÇ Deskripsi                     ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ Probable Singleton       ‚îÇ {n_singleton:>8,}   ‚îÇ DNCN < Q25={q25:.2f}           ‚îÇ\n",
    "‚îÇ (CGA Candidate)          ‚îÇ ({n_singleton/n_noise_base:.1%})      ‚îÇ Dekat cluster, bisa di-augment‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ Borderline               ‚îÇ {n_borderline:>8,}   ‚îÇ Q25‚ÄìQ75                       ‚îÇ\n",
    "‚îÇ                          ‚îÇ ({n_borderline/n_noise_base:.1%})      ‚îÇ Tidak bisa ditentukan         ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ Probable Outlier         ‚îÇ {n_outlier:>8,}   ‚îÇ DNCN > Q75={q75:.2f}           ‚îÇ\n",
    "‚îÇ (Low Quality)            ‚îÇ ({n_outlier/n_noise_base:.1%})      ‚îÇ Jauh dari semua cluster       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")\n",
    "\n",
    "cga_candidates = n_singleton + n_borderline // 2\n",
    "print(f\"  Estimasi CGA candidates (Singleton + sebagian Borderline): ~{cga_candidates:,}\")\n",
    "print(f\"  Dari {n_noise_base:,} noise points, ~{cga_candidates/n_noise_base:.1%} berpotensi dibantu oleh CGA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell7-md",
   "metadata": {},
   "source": [
    "## Cell 7 ‚Äî Iterative Re-Clustering Noise\n",
    "\n",
    "Strategi ini memisahkan noise points dan menjalankan HDBSCAN ulang **hanya pada noise** dengan `min_cluster_size` yang jauh lebih kecil.\n",
    "\n",
    "Logika: noise yang HDBSCAN buang bukan karena embedding-nya tidak baik, melainkan karena threshold minimum cluster terlalu tinggi. Dengan threshold yang lebih rendah, beberapa grup kecil bisa terbentuk.\n",
    "\n",
    "**Hasil re-clustering** kemudian digabung dengan clustering original untuk mendapatkan label final yang memiliki coverage lebih tinggi tanpa mengubah kualitas cluster yang sudah ada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üî¨ EKSPERIMEN: ITERATIVE RE-CLUSTERING NOISE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n  Input: {n_noise_base:,} noise points dari baseline\")\n",
    "print(f\"  Dimensi: {noise_emb.shape}\")\n",
    "\n",
    "refit_results = []\n",
    "\n",
    "n_clusters_original = len(set(labels_baseline)) - 1\n",
    "label_offset        = n_clusters_original\n",
    "\n",
    "print(f\"\\n  Label offset: +{label_offset} (agar tidak bentrok dengan label asli)\")\n",
    "print(f\"\\n  {'mcs':>4} {'ms':>4} | {'new_clusters':>12} {'recovered':>10} {'coverage_gain':>14}\")\n",
    "print(f\"  {'‚îÄ'*4} {'‚îÄ'*4} | {'‚îÄ'*12} {'‚îÄ'*10} {'‚îÄ'*14}\")\n",
    "\n",
    "for mcs in Config.RECLUSTERING_MCS_LIST:\n",
    "    for ms in Config.RECLUSTERING_MS_LIST:\n",
    "        if ms > mcs:\n",
    "            continue\n",
    "\n",
    "        re_clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size         = mcs,\n",
    "            min_samples              = ms,\n",
    "            cluster_selection_method = 'eom',\n",
    "            metric                   = 'correlation',\n",
    "        )\n",
    "        re_labels = re_clusterer.fit_predict(noise_emb)\n",
    "\n",
    "        n_new_clusters = len(set(re_labels)) - (1 if -1 in re_labels else 0)\n",
    "        n_recovered    = int((re_labels != -1).sum())\n",
    "        coverage_gain  = n_recovered / len(embeddings)\n",
    "\n",
    "        re_labels_offset = np.where(re_labels == -1, -1, re_labels + label_offset)\n",
    "\n",
    "        labels_combined = labels_baseline.copy()\n",
    "        labels_combined[noise_mask] = re_labels_offset\n",
    "\n",
    "        mask_combined  = labels_combined != -1\n",
    "        coverage_total = float(mask_combined.sum() / len(labels_combined))\n",
    "\n",
    "        sil = None\n",
    "        if mask_combined.sum() > 10 and len(set(labels_combined[mask_combined])) >= 2:\n",
    "            sil = float(silhouette_score(\n",
    "                embeddings[mask_combined], labels_combined[mask_combined],\n",
    "                metric='correlation', sample_size=3000, random_state=42\n",
    "            ))\n",
    "\n",
    "        refit_results.append({\n",
    "            'mcs'            : mcs,\n",
    "            'ms'             : ms,\n",
    "            'n_new_clusters' : n_new_clusters,\n",
    "            'n_recovered'    : n_recovered,\n",
    "            'coverage_gain'  : coverage_gain,\n",
    "            'coverage_total' : coverage_total,\n",
    "            'silhouette'     : sil,\n",
    "            'labels_combined': labels_combined.copy(),\n",
    "        })\n",
    "\n",
    "        sil_str = f\"{sil:.4f}\" if sil else \"  N/A\"\n",
    "        print(f\"  {mcs:>4} {ms:>4} | {n_new_clusters:>12} {n_recovered:>10,} \"\n",
    "              f\"{coverage_gain:>14.1%}  (total cov: {coverage_total:.1%}, sil: {sil_str})\")\n",
    "\n",
    "df_refit = pd.DataFrame([{k: v for k, v in r.items() if k != 'labels_combined'}\n",
    "                          for r in refit_results])\n",
    "df_valid_refit = df_refit.dropna(subset=['silhouette'])\n",
    "\n",
    "best_refit = df_valid_refit.loc[df_valid_refit['coverage_total'].idxmax()]\n",
    "print(f\"\\n  üèÜ Best re-clustering: mcs={int(best_refit['mcs'])}, ms={int(best_refit['ms'])}\")\n",
    "print(f\"     New clusters: {int(best_refit['n_new_clusters'])}\")\n",
    "print(f\"     Recovered   : {int(best_refit['n_recovered']):,} noise points\")\n",
    "print(f\"     Coverage gain: +{best_refit['coverage_gain']:.1%}\")\n",
    "print(f\"     Total coverage: {best_refit['coverage_total']:.1%}\")\n",
    "print(f\"     Silhouette (combined): {best_refit['silhouette']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell8-md",
   "metadata": {},
   "source": [
    "## Cell 8 ‚Äî Visualisasi Komposisi Noise\n",
    "\n",
    "Empat panel visualisasi:\n",
    "1. **Pie chart klasifikasi noise** ‚Äî proporsi singleton vs borderline vs outlier\n",
    "2. **Histogram DNCN** ‚Äî distribusi jarak noise ke cluster terdekat\n",
    "3. **Coverage comparison** ‚Äî baseline vs setelah re-clustering terbaik\n",
    "4. **Distribusi ukuran cluster baru** dari re-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "ax_pie, ax_hist, ax_bar, ax_newclust = axes.flatten()\n",
    "\n",
    "pie_labels = [\n",
    "    f'CGA Candidate\\n(Singleton)\\n{n_singleton:,} ({n_singleton/n_noise_base:.1%})',\n",
    "    f'Borderline\\n{n_borderline:,} ({n_borderline/n_noise_base:.1%})',\n",
    "    f'Low Quality\\n(Outlier)\\n{n_outlier:,} ({n_outlier/n_noise_base:.1%})',\n",
    "]\n",
    "pie_sizes  = [n_singleton, n_borderline, n_outlier]\n",
    "pie_colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "ax_pie.pie(pie_sizes, labels=pie_labels, colors=pie_colors,\n",
    "           autopct='%1.1f%%', startangle=90,\n",
    "           wedgeprops={'edgecolor': 'white', 'linewidth': 2})\n",
    "ax_pie.set_title('Klasifikasi Noise Points\\n(berdasarkan DNCN)', fontweight='bold')\n",
    "\n",
    "ax_hist.hist(dncn_all, bins=50, color='#3498db', edgecolor='white', alpha=0.8)\n",
    "ax_hist.axvline(q25, color='#2ecc71', linestyle='--', linewidth=2, label=f'Q25 = {q25:.2f}')\n",
    "ax_hist.axvline(q75, color='#e74c3c', linestyle='--', linewidth=2, label=f'Q75 = {q75:.2f}')\n",
    "ax_hist.set_xlabel('Jarak ke Nearest Clustered Neighbor (DNCN)')\n",
    "ax_hist.set_ylabel('Frekuensi')\n",
    "ax_hist.set_title('Distribusi DNCN Noise Points', fontweight='bold')\n",
    "ax_hist.legend()\n",
    "\n",
    "bar_labels = ['Baseline\\n(tanpa re-cluster)', f'+ Re-Cluster\\n(mcs={int(best_refit[\"mcs\"])}, ms={int(best_refit[\"ms\"])})']\n",
    "cov_vals   = [coverage_base, best_refit['coverage_total']]\n",
    "bar_colors = ['steelblue', 'darkorange']\n",
    "bars = ax_bar.bar(bar_labels, [v*100 for v in cov_vals], color=bar_colors, edgecolor='black')\n",
    "ax_bar.set_ylabel('Coverage Rate (%)')\n",
    "ax_bar.set_title('Coverage Rate Sebelum vs Sesudah\\nIterative Re-Clustering', fontweight='bold')\n",
    "ax_bar.set_ylim(0, 100)\n",
    "for bar, val in zip(bars, cov_vals):\n",
    "    ax_bar.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{val:.1%}', ha='center', fontsize=12, fontweight='bold')\n",
    "delta_cov = best_refit['coverage_total'] - coverage_base\n",
    "ax_bar.text(0.5, 0.85, f'Œî = {delta_cov:+.1%}', transform=ax_bar.transAxes,\n",
    "            ha='center', fontsize=14, fontweight='bold',\n",
    "            color='green' if delta_cov > 0 else 'red')\n",
    "\n",
    "best_refit_result = refit_results[df_valid_refit['coverage_total'].argmax()]\n",
    "best_combined_labels = best_refit_result['labels_combined']\n",
    "noise_re_labels = best_combined_labels[noise_mask]\n",
    "new_cluster_labels = noise_re_labels[noise_re_labels != -1]\n",
    "\n",
    "if len(new_cluster_labels) > 0:\n",
    "    new_cluster_sizes = Counter(new_cluster_labels)\n",
    "    sizes = list(new_cluster_sizes.values())\n",
    "    ax_newclust.hist(sizes, bins=min(20, len(set(sizes))), color='#9b59b6', edgecolor='white', alpha=0.8)\n",
    "    ax_newclust.set_xlabel('Ukuran cluster baru')\n",
    "    ax_newclust.set_ylabel('Frekuensi')\n",
    "    ax_newclust.set_title(f'Distribusi Ukuran Cluster Baru dari Re-Clustering\\n({int(best_refit[\"n_new_clusters\"])} cluster baru)', fontweight='bold')\n",
    "    ax_newclust.axvline(np.mean(sizes), color='red', linestyle='--', label=f'Mean: {np.mean(sizes):.1f}')\n",
    "    ax_newclust.legend()\n",
    "else:\n",
    "    ax_newclust.text(0.5, 0.5, 'Tidak ada cluster baru', ha='center', va='center',\n",
    "                     transform=ax_newclust.transAxes, fontsize=14)\n",
    "    ax_newclust.set_title('Distribusi Ukuran Cluster Baru', fontweight='bold')\n",
    "\n",
    "fig.suptitle('Notebook 11: Analisis Komposisi Noise', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_path = f\"{Config.PLOTS_DIR}notebook11_noise_analysis.png\"\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"üíæ Plot disimpan: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell9-md",
   "metadata": {},
   "source": [
    "## Cell 9 ‚Äî Summary Final & Implikasi untuk CGA\n",
    "\n",
    "Summary ini menjadi \"kesimpulan\" analisis pra-CGA yang akan dikutip di skripsi untuk memotivasi pendekatan CGA di Eksperimen 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìã NOTEBOOK 11 ‚Äî SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              ANALISIS KOMPOSISI NOISE ‚Äî RINGKASAN                    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  DATASET:                                                            ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Total embeddings: {len(embeddings):,}                             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Noise (baseline): {n_noise_base:,} ({1-coverage_base:.1%})        ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Coverage baseline: {coverage_base:.1%}                            ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  KLASIFIKASI NOISE (berdasarkan DNCN):                               ‚îÇ\n",
    "‚îÇ  ‚Ä¢ CGA Candidate (Singleton)  : {n_singleton:,} ({n_singleton/n_noise_base:.1%})  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Borderline                 : {n_borderline:,} ({n_borderline/n_noise_base:.1%})  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Low Quality (Outlier)      : {n_outlier:,} ({n_outlier/n_noise_base:.1%})  ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ  ITERATIVE RE-CLUSTERING:                                            ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Best config: mcs={int(best_refit['mcs'])}, ms={int(best_refit['ms'])}                              ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Cluster baru dibentuk  : {int(best_refit['n_new_clusters'])}                      ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Noise points recovered : {int(best_refit['n_recovered']):,}                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Coverage gain          : +{best_refit['coverage_gain']:.1%}                       ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Total coverage setelah : {best_refit['coverage_total']:.1%}                       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")\n",
    "\n",
    "cga_justified = n_singleton / n_noise_base >= 0.3\n",
    "\n",
    "print(\"\\n  IMPLIKASI UNTUK CGA:\")\n",
    "if cga_justified:\n",
    "    print(f\"  ‚úÖ CGA JUSTIFIED\")\n",
    "    print(f\"  {n_singleton/n_noise_base:.1%} dari noise adalah probable singletons (CGA candidates).\")\n",
    "    print(f\"  Ini berarti ada ~{n_singleton:,} wajah yang embedding-nya valid\")\n",
    "    print(f\"  tapi tidak memiliki cukup sampel untuk membentuk cluster.\")\n",
    "    print(f\"  CGA dapat mensintesis embedding tambahan untuk orang-orang ini.\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è CGA KURANG JUSTIFIED\")\n",
    "    print(f\"  Hanya {n_singleton/n_noise_base:.1%} noise yang merupakan probable singletons.\")\n",
    "    print(f\"  Mayoritas noise ({n_outlier/n_noise_base:.1%}) kemungkinan adalah outlier kualitas rendah.\")\n",
    "    print(f\"  CGA pada data ini mungkin tidak signifikan meningkatkan coverage.\")\n",
    "\n",
    "save_data = {\n",
    "    'timestamp'            : datetime.now().isoformat(),\n",
    "    'notebook'             : 'Notebook 11: Noise Composition Analysis',\n",
    "    'n_total'              : len(embeddings),\n",
    "    'n_noise_baseline'     : n_noise_base,\n",
    "    'coverage_baseline'    : coverage_base,\n",
    "    'dncn_all'             : dncn_all,\n",
    "    'q25_dncn'             : q25,\n",
    "    'q75_dncn'             : q75,\n",
    "    'n_singleton'          : int(n_singleton),\n",
    "    'n_borderline'         : int(n_borderline),\n",
    "    'n_outlier'            : int(n_outlier),\n",
    "    'singleton_mask'       : singleton_mask,\n",
    "    'outlier_mask'         : outlier_mask,\n",
    "    'noise_indices'        : noise_indices,\n",
    "    'refit_results'        : [{k: v for k, v in r.items() if k != 'labels_combined'}\n",
    "                               for r in refit_results],\n",
    "    'best_combined_labels' : best_refit_result['labels_combined'],\n",
    "    'best_refit_params'    : {'mcs': int(best_refit['mcs']), 'ms': int(best_refit['ms'])},\n",
    "    'best_total_coverage'  : float(best_refit['coverage_total']),\n",
    "    'cga_justified'        : cga_justified,\n",
    "}\n",
    "\n",
    "pkl_path = f\"{Config.RESULTS_DIR}notebook11_noise_analysis.pkl\"\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump(save_data, f)\n",
    "\n",
    "print(f\"\\n\\nüíæ Disimpan: {pkl_path}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ NEXT: Eksperimen 2 ‚Äî CGA Augmentation\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "   Input untuk CGA:\n",
    "   ‚Ä¢ Baseline labels    : {coverage_base:.1%} coverage\n",
    "   ‚Ä¢ CGA candidates     : ~{n_singleton:,} noise points\n",
    "   ‚Ä¢ Best metric found  : hasil dari Notebook 9 (UMAP) atau 10 (Angular)\n",
    "   ‚Ä¢ Combined coverage  : {best_refit['coverage_total']:.1%} (setelah re-clustering)\n",
    "\"\"\")"
   ]
  }
 ]
}
